{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to DART DART is the Digital Archivist's Resource Tool. It provides both a GUI and a command-line interface for packaging files and uploading them to remote repositories. Download Updated October 27, 2021 Download the DART installer for your system. The current version is 2.0.16 (released October 27, 2021). Mac : https://s3.amazonaws.com/aptrust.public.download/DART/DART-2.0.16.dmg Windows : https://s3.amazonaws.com/aptrust.public.download/DART/DART+Setup+2.0.16.exe Linux : https://s3.amazonaws.com/aptrust.public.download/DART/DART_2.0.16_amd64.deb Double-click the installer after download and follow the prompts on screen. After installation, check out the Getting Started page Supported Operations The current release of DART 2.0.16 supports the following features: Creating BagIt bags that conform to defined BagIt profiles Validating BagIt bags according to defined BagIt profiles Uploading bags and other files to remote S3 and SFTP endpoints Creating and modifying BagIt profiles through a visual point-and-click editor Defining repeatable Workflows for bagging and uploading files Running multiple simultaneous bagging and upload jobs Read-only integration with the APTrust's REST API to display the status of ingested materials and pending work items A command-line tool to enable scriptable bagging and upload operations BagIt Profile import and export BagIt Profile customization throug a visual editor Settings import and export Workflow export To start using DART, see our Getting Started page. Note In the future, DART plans to support running workflows on servers. Currently, this is difficult because DART's underlying Electron framework insists on graphics capabilities even when running in command-line mode. We plan to release a stand-alone binary that will be able to run DART jobs and workflows on a server without requiring a UI or graphics capabilities. You'll find a draft of the proposed features here . You can provide feedback through the survey at the end of the features document. Plugin Architecture Most of DART's features are implemented in plugins, which enable developers to add new features without having to understand all of DART's internals. DART is an open source project of the Academic Preservation Trust, which encourages developers to contribute new plugins to extend the tool's functionality. DART 2.0 supports the following types of plugins: Format Readers - These allow DART to read files packaged in various formats, such as tar, zip, rar, parchive, OCFL, etc. Currently supported in formats in version 2.0: directory/file system tar Format Writers - These allow DART to write files in various formats, such as tar, zip, rar, parchive, OCFL, etc. Currently supported in formats in version 2.0: directory/file system tar Network Clients - These allow DART to send and retrieve files across a network. DART 2.0 supports the following protocols: S3 SFTP Repository Clients - These allow DART to interact with remote repositories. Currently supported: APTrust Writing DART plugins requires a working knowledge of JavaScript and HTML. If you're interested in developing DART plugins, see our Developers page and our full API documentation . Useful Links Getting Started with DART Developing DART Plugins DART API documentation DART source code on GitHub Video: Jobs and Workflows Video: BagIt Profiles in DART Video: Settings Import and Export Credits Brace yourselves gentlemen. According to the gas chromatograph, the secret ingredient is... Love!? Who's been screwing with this thing?","title":"Home"},{"location":"#welcome-to-dart","text":"DART is the Digital Archivist's Resource Tool. It provides both a GUI and a command-line interface for packaging files and uploading them to remote repositories.","title":"Welcome to DART"},{"location":"#download","text":"Updated October 27, 2021 Download the DART installer for your system. The current version is 2.0.16 (released October 27, 2021). Mac : https://s3.amazonaws.com/aptrust.public.download/DART/DART-2.0.16.dmg Windows : https://s3.amazonaws.com/aptrust.public.download/DART/DART+Setup+2.0.16.exe Linux : https://s3.amazonaws.com/aptrust.public.download/DART/DART_2.0.16_amd64.deb Double-click the installer after download and follow the prompts on screen. After installation, check out the Getting Started page","title":"Download"},{"location":"#supported-operations","text":"The current release of DART 2.0.16 supports the following features: Creating BagIt bags that conform to defined BagIt profiles Validating BagIt bags according to defined BagIt profiles Uploading bags and other files to remote S3 and SFTP endpoints Creating and modifying BagIt profiles through a visual point-and-click editor Defining repeatable Workflows for bagging and uploading files Running multiple simultaneous bagging and upload jobs Read-only integration with the APTrust's REST API to display the status of ingested materials and pending work items A command-line tool to enable scriptable bagging and upload operations BagIt Profile import and export BagIt Profile customization throug a visual editor Settings import and export Workflow export To start using DART, see our Getting Started page. Note In the future, DART plans to support running workflows on servers. Currently, this is difficult because DART's underlying Electron framework insists on graphics capabilities even when running in command-line mode. We plan to release a stand-alone binary that will be able to run DART jobs and workflows on a server without requiring a UI or graphics capabilities. You'll find a draft of the proposed features here . You can provide feedback through the survey at the end of the features document.","title":"Supported Operations"},{"location":"#plugin-architecture","text":"Most of DART's features are implemented in plugins, which enable developers to add new features without having to understand all of DART's internals. DART is an open source project of the Academic Preservation Trust, which encourages developers to contribute new plugins to extend the tool's functionality. DART 2.0 supports the following types of plugins: Format Readers - These allow DART to read files packaged in various formats, such as tar, zip, rar, parchive, OCFL, etc. Currently supported in formats in version 2.0: directory/file system tar Format Writers - These allow DART to write files in various formats, such as tar, zip, rar, parchive, OCFL, etc. Currently supported in formats in version 2.0: directory/file system tar Network Clients - These allow DART to send and retrieve files across a network. DART 2.0 supports the following protocols: S3 SFTP Repository Clients - These allow DART to interact with remote repositories. Currently supported: APTrust Writing DART plugins requires a working knowledge of JavaScript and HTML. If you're interested in developing DART plugins, see our Developers page and our full API documentation .","title":"Plugin Architecture"},{"location":"#useful-links","text":"Getting Started with DART Developing DART Plugins DART API documentation DART source code on GitHub Video: Jobs and Workflows Video: BagIt Profiles in DART Video: Settings Import and Export","title":"Useful Links"},{"location":"#credits","text":"Brace yourselves gentlemen. According to the gas chromatograph, the secret ingredient is... Love!? Who's been screwing with this thing?","title":"Credits"},{"location":"download/","text":"Updated October 27, 2021 Download the DART installer for your system. The current version is 2.0.16 (released October 27, 2021). Mac : https://s3.amazonaws.com/aptrust.public.download/DART/DART-2.0.16.dmg Windows : https://s3.amazonaws.com/aptrust.public.download/DART/DART+Setup+2.0.16.exe Linux : https://s3.amazonaws.com/aptrust.public.download/DART/DART_2.0.16_amd64.deb Double-click the installer after download and follow the prompts on screen. After installation, check out the Getting Started page","title":"Download"},{"location":"videos/","text":"DART Videos Jobs and Workflows This video covers: How to set up and run a job. How to create a workflow to build and upload lots of bags. Working with BagIt Profiles This video covers: How to import a BagIt profile. How to customize a BagIt profile using the visual editor. How to export a BagIt profile. Importing and Exporting Settings This video covers: How to export settings to share with other users. How to import settings shared by other users.","title":"Videos"},{"location":"videos/#dart-videos","text":"","title":"DART Videos"},{"location":"videos/#jobs-and-workflows","text":"This video covers: How to set up and run a job. How to create a workflow to build and upload lots of bags.","title":"Jobs and Workflows"},{"location":"videos/#working-with-bagit-profiles","text":"This video covers: How to import a BagIt profile. How to customize a BagIt profile using the visual editor. How to export a BagIt profile.","title":"Working with BagIt Profiles"},{"location":"videos/#importing-and-exporting-settings","text":"This video covers: How to export settings to share with other users. How to import settings shared by other users.","title":"Importing and Exporting Settings"},{"location":"developers/","text":"Developer's Guide to DART DART's primary purpose is to pack and ship digital materials for preservation. It includes a GUI for non-technical users who want to drag and drop files, and a command line interface for more technical users who want to script DART jobs . While DART's initial release supports the BagIt packaging format and uploads to S3-compatible services, future users may require additional package formats such as rar, parchive, OCFL, etc. They may also need to ship files using network protocols such as SFTP, rsync, scp and others. DART has a plugin architecture that allows developers to contribute code that will add these package formats and network protocols. If you know how to write JavaScript, you can contribute plugins without having to understand DART's internals. Getting the Code To get started developing DART plugins, download the source from GitHub. git clone https://github.com/APTrust/dart.git Once you have the source, you'll need to install the dependencies. Change into the dart directory and run this: npm install To ensure all is working, run the tests. npm test -- --runInBand Useful Links Writing Plugins for DART Building DART Installers DART API Documentation DART source on GitHub","title":"Developer's Guide to DART"},{"location":"developers/#developers-guide-to-dart","text":"DART's primary purpose is to pack and ship digital materials for preservation. It includes a GUI for non-technical users who want to drag and drop files, and a command line interface for more technical users who want to script DART jobs . While DART's initial release supports the BagIt packaging format and uploads to S3-compatible services, future users may require additional package formats such as rar, parchive, OCFL, etc. They may also need to ship files using network protocols such as SFTP, rsync, scp and others. DART has a plugin architecture that allows developers to contribute code that will add these package formats and network protocols. If you know how to write JavaScript, you can contribute plugins without having to understand DART's internals.","title":"Developer's Guide to DART"},{"location":"developers/#getting-the-code","text":"To get started developing DART plugins, download the source from GitHub. git clone https://github.com/APTrust/dart.git Once you have the source, you'll need to install the dependencies. Change into the dart directory and run this: npm install To ensure all is working, run the tests. npm test -- --runInBand","title":"Getting the Code"},{"location":"developers/#useful-links","text":"Writing Plugins for DART Building DART Installers DART API Documentation DART source on GitHub","title":"Useful Links"},{"location":"developers/architecture/","text":"Architecture DART was designed around a few core principles. It should use commonly available and well understood open source technologies. It should run on Windows, Mac, and Linux. It should include a plugin architecture so developers can add new features and functions without having to learn the system's internals. It should separate core functional code from UI code, so the application's core features can be run from the UI or from the command line. Components should be as loosely coupled as possible so that: They can be replaced at will, and so They don't cause bugs in other components. All components should be implemented in the simplest way possible. This means: Don't add extraneous JavaScript libraries just to pick up one or two handy functions. Don't do in JavaScript what the browser already does for you (such as navigation and making decisions about rendering). Don't use libraries like React, Angular, Vue, etc. (because DART's HTML is so simple, these libraries would only add complexity without adding value). DART should be maintainable for the long term. In practice, this goes beyond architectural principles to include two ongoing practices to accompany all new and updated code: Thorough testing. Complete and useful documentation. Tech Stack DART is built on the following tech stack: Electron , which provides a cross-platform UI as well as build and distribution capabilities. Node.js , which provides a portable JavaScript runtime and a set of core libraries. HTML5 , which provides simple interactive displays. Bootstrap 4 , which provides a rich and flexible CSS framework. Handlebars , a brain-dead templating language that forces you to separate logic from presentation. MVC Pattern DART uses an MVC design pattern similar to Ruby on Rails. This choice was guided by the following considerations: DART can route requests using standard URLs. There's no need for a routing engine, or for JavaScript to keep track of navigation history, the state of HTML components on the page, data mappings, a virtual DOM, etc. Each MVC request routes to one endpoint of one controller and the mapping is always obvious from the URL. For example, AppSetting/list maps to the list() method of the AppSetting controller. All request parameters are passed through the URL, so there's no need for special JavaScript classes, structures, or mappings. Each request results in a full re-rendering of the DOM. This is seen as an evil to be avoided in web applications because a page load requires one or more network calls and causes the user to lose state information and the dynamically loaded content that comes from infinite scrolling. DART does not need to make network calls to load pages, and because it keeps no state, there's nothing to lose. Because a typical DART page loads only 10-20 kilobytes of HTML, it renders quickly, taking advantage of all the browser's rendering optimizations.","title":"Architecture"},{"location":"developers/architecture/#architecture","text":"DART was designed around a few core principles. It should use commonly available and well understood open source technologies. It should run on Windows, Mac, and Linux. It should include a plugin architecture so developers can add new features and functions without having to learn the system's internals. It should separate core functional code from UI code, so the application's core features can be run from the UI or from the command line. Components should be as loosely coupled as possible so that: They can be replaced at will, and so They don't cause bugs in other components. All components should be implemented in the simplest way possible. This means: Don't add extraneous JavaScript libraries just to pick up one or two handy functions. Don't do in JavaScript what the browser already does for you (such as navigation and making decisions about rendering). Don't use libraries like React, Angular, Vue, etc. (because DART's HTML is so simple, these libraries would only add complexity without adding value). DART should be maintainable for the long term. In practice, this goes beyond architectural principles to include two ongoing practices to accompany all new and updated code: Thorough testing. Complete and useful documentation.","title":"Architecture"},{"location":"developers/architecture/#tech-stack","text":"DART is built on the following tech stack: Electron , which provides a cross-platform UI as well as build and distribution capabilities. Node.js , which provides a portable JavaScript runtime and a set of core libraries. HTML5 , which provides simple interactive displays. Bootstrap 4 , which provides a rich and flexible CSS framework. Handlebars , a brain-dead templating language that forces you to separate logic from presentation.","title":"Tech Stack"},{"location":"developers/architecture/#mvc-pattern","text":"DART uses an MVC design pattern similar to Ruby on Rails. This choice was guided by the following considerations: DART can route requests using standard URLs. There's no need for a routing engine, or for JavaScript to keep track of navigation history, the state of HTML components on the page, data mappings, a virtual DOM, etc. Each MVC request routes to one endpoint of one controller and the mapping is always obvious from the URL. For example, AppSetting/list maps to the list() method of the AppSetting controller. All request parameters are passed through the URL, so there's no need for special JavaScript classes, structures, or mappings. Each request results in a full re-rendering of the DOM. This is seen as an evil to be avoided in web applications because a page load requires one or more network calls and causes the user to lose state information and the dynamically loaded content that comes from infinite scrolling. DART does not need to make network calls to load pages, and because it keeps no state, there's nothing to lose. Because a typical DART page loads only 10-20 kilobytes of HTML, it renders quickly, taking advantage of all the browser's rendering optimizations.","title":"MVC Pattern"},{"location":"developers/building/","text":"Building DART Once you've cloned the DART source code from GitHub , there are two ways to build the application. cd into the DART project directory and run the command ./node_modules/.bin/electron-builder . This will run a command-line build for your platform. cd into the DART project directory and run the command npm run electron-toolkit . This will present a GUI that makes it easy to customize your build and to build DART for other platforms.","title":"Building"},{"location":"developers/building/#building-dart","text":"Once you've cloned the DART source code from GitHub , there are two ways to build the application. cd into the DART project directory and run the command ./node_modules/.bin/electron-builder . This will run a command-line build for your platform. cd into the DART project directory and run the command npm run electron-toolkit . This will present a GUI that makes it easy to customize your build and to build DART for other platforms.","title":"Building DART"},{"location":"developers/data_storage/","text":"Data Storage DART stores all of its settings data in plain text JSON files. You find these files in the DART data directory. Choose Help > About from the DART menu to find the data directory on your system. You'll see a dialog like the following. DART uses the simple conf module to read, write, and search data, and write-file-atomic to prevent write conflicts. If you want to dig further into DART's data persistence, see the documentation for JsonStore and PersistentObject . You can examine DART's data stores by opening the JSON files, or in the console by following these steps: Open DART by running npm start from the DART project directory. Right click anywhere on the page and choose Inspect Element from the context menu. Swich to the Console view in the developer tools window. Type DART.Core.Context.dataStores in the console.","title":"Data Storage"},{"location":"developers/data_storage/#data-storage","text":"DART stores all of its settings data in plain text JSON files. You find these files in the DART data directory. Choose Help > About from the DART menu to find the data directory on your system. You'll see a dialog like the following. DART uses the simple conf module to read, write, and search data, and write-file-atomic to prevent write conflicts. If you want to dig further into DART's data persistence, see the documentation for JsonStore and PersistentObject . You can examine DART's data stores by opening the JSON files, or in the console by following these steps: Open DART by running npm start from the DART project directory. Right click anywhere on the page and choose Inspect Element from the context menu. Swich to the Console view in the developer tools window. Type DART.Core.Context.dataStores in the console.","title":"Data Storage"},{"location":"developers/documenting/","text":"Documenting DART If you contribute code to DART, please document it for the sanity of the people who will maintain it. DART uses JSDoc to generate the DART API documentation . Look at the source code for examples of how we use JSDoc. In addition to documenting your API methods, you should comment sections of your code in which the logic or behavior is not obvious. Always code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. -- John F. Woods","title":"Documenting"},{"location":"developers/documenting/#documenting-dart","text":"If you contribute code to DART, please document it for the sanity of the people who will maintain it. DART uses JSDoc to generate the DART API documentation . Look at the source code for examples of how we use JSDoc. In addition to documenting your API methods, you should comment sections of your code in which the logic or behavior is not obvious. Always code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. -- John F. Woods","title":"Documenting DART"},{"location":"developers/job_flow/","text":"Job Flow All DART jobs are run as individual processes. When you run a job from the DART UI, the application forks a child process and runs the job in that process. The code that forks the child is in the run method of the JobRunController , which is documented here . Forking child processes has a number of advantages over running jobs in the same process as the UI. You can run multiple jobs simultaneously. You can continue to use the UI while jobs run. A crashed job will not crash the UI. The downside of running jobs in child processes is that if you close the DART application, you kill all of the running jobs. This is why DART always displays a badge in the upper right corner when jobs are running. All DART jobs, whether run from the UI or from the command line, are launched from the run() method of main.js , which does the following: Calls the JobLoader to load the job. Info The JobLoader figures out whether you're passing in a Job object or some other type of object from which it can construct a job. You'll typically pass in JSON representing a lightweight JobParams object and the JobLoader will construct the job from that. The JobParams object simply names a set of files, a Workflow to pass them through, and a set of metadata tags to add to the package. The user documentaiton for JobParams will be helpful to developers. Passes the Job object to the JobRunner and then calls the JobRunner's run method. Returns the exit code so that the script or application that launched the job can know whether it succeeded. An exit code of zero indicates success. Any other code indicates failure. These codes are spelled out in core/constants.js as follows: /** * This exit code indicates a process completed successfully, * with no errors. * * @type {number} */ EXIT_SUCCESS : 0 , /** * This exit code indicates a process ran to completion, but one * or more errors occurred along the way. * * @type {number} */ EXIT_COMPLETED_WITH_ERRORS : 1 , /** * This exit code indicates a process did not complete * due to invalid parameters. * * @type {number} */ EXIT_INVALID_PARAMS : 2 , /** * This exit code indicates that the process did not complete * due to an unexpected runtime error. * * @type {number} */ EXIT_RUNTIME_ERROR : 3 , The JobRunner, Step by Step The JobRunner does the following: If the Job includes a PackageOperation , the JobRunner packages the files specified in the operation. The PackageOperation describes how DART should create a bag, or a tar file, or a zip file, or some other package. If a user wants to upload files without packaging, there will be no PackageOperation. If the PackageOperation produced a BagIt bag (tarred or not), DART validates the bag. The JobRunner will create a ValidationOperation to describe what needs to be validated and how. If the job includes any UploadOperations , DART will execute them one at a time, in sequence. The JobRunner sets and returns the appropriate exit code. (Note that the most common cause of the EXIT_COMPLETED_WITH_ERRORS code is a job whose packaging and validation succeeded, along with some but not all of the upload operations.) The JobRunner logs its work, and you can watch the operations unfold in the logs . Each of the objects PackageOperation , ValidationOperation , and UploadOperation include an OperationResult object. After a job is complete, each of the operations and their results are stored with the job record in the DART's jobs database, which is a plain text JSON file. You can open the file directly to examine the full state of the object. See Data Storage for more information. All of the code for running jobs lives in the workers folder of the DART source. The four key files have the following tasks: workers/job_runner.js orchestrates the work of bagging, validating, and uploading. Links: source code , documentation , tests . workers/bag_creator.js creates bags for the PackageOperation step of the job. Links: source code , documentation , tests . workers/bag_validator.js validates that a bag is complete and conforms to a specified BagIt profile. Links: source , documentation , tests . workers/uploader.js uploads files to a remote upload target. Links: source , documentation , tests .","title":"Job Flow"},{"location":"developers/job_flow/#job-flow","text":"All DART jobs are run as individual processes. When you run a job from the DART UI, the application forks a child process and runs the job in that process. The code that forks the child is in the run method of the JobRunController , which is documented here . Forking child processes has a number of advantages over running jobs in the same process as the UI. You can run multiple jobs simultaneously. You can continue to use the UI while jobs run. A crashed job will not crash the UI. The downside of running jobs in child processes is that if you close the DART application, you kill all of the running jobs. This is why DART always displays a badge in the upper right corner when jobs are running. All DART jobs, whether run from the UI or from the command line, are launched from the run() method of main.js , which does the following: Calls the JobLoader to load the job. Info The JobLoader figures out whether you're passing in a Job object or some other type of object from which it can construct a job. You'll typically pass in JSON representing a lightweight JobParams object and the JobLoader will construct the job from that. The JobParams object simply names a set of files, a Workflow to pass them through, and a set of metadata tags to add to the package. The user documentaiton for JobParams will be helpful to developers. Passes the Job object to the JobRunner and then calls the JobRunner's run method. Returns the exit code so that the script or application that launched the job can know whether it succeeded. An exit code of zero indicates success. Any other code indicates failure. These codes are spelled out in core/constants.js as follows: /** * This exit code indicates a process completed successfully, * with no errors. * * @type {number} */ EXIT_SUCCESS : 0 , /** * This exit code indicates a process ran to completion, but one * or more errors occurred along the way. * * @type {number} */ EXIT_COMPLETED_WITH_ERRORS : 1 , /** * This exit code indicates a process did not complete * due to invalid parameters. * * @type {number} */ EXIT_INVALID_PARAMS : 2 , /** * This exit code indicates that the process did not complete * due to an unexpected runtime error. * * @type {number} */ EXIT_RUNTIME_ERROR : 3 ,","title":"Job Flow"},{"location":"developers/job_flow/#the-jobrunner-step-by-step","text":"The JobRunner does the following: If the Job includes a PackageOperation , the JobRunner packages the files specified in the operation. The PackageOperation describes how DART should create a bag, or a tar file, or a zip file, or some other package. If a user wants to upload files without packaging, there will be no PackageOperation. If the PackageOperation produced a BagIt bag (tarred or not), DART validates the bag. The JobRunner will create a ValidationOperation to describe what needs to be validated and how. If the job includes any UploadOperations , DART will execute them one at a time, in sequence. The JobRunner sets and returns the appropriate exit code. (Note that the most common cause of the EXIT_COMPLETED_WITH_ERRORS code is a job whose packaging and validation succeeded, along with some but not all of the upload operations.) The JobRunner logs its work, and you can watch the operations unfold in the logs . Each of the objects PackageOperation , ValidationOperation , and UploadOperation include an OperationResult object. After a job is complete, each of the operations and their results are stored with the job record in the DART's jobs database, which is a plain text JSON file. You can open the file directly to examine the full state of the object. See Data Storage for more information. All of the code for running jobs lives in the workers folder of the DART source. The four key files have the following tasks: workers/job_runner.js orchestrates the work of bagging, validating, and uploading. Links: source code , documentation , tests . workers/bag_creator.js creates bags for the PackageOperation step of the job. Links: source code , documentation , tests . workers/bag_validator.js validates that a bag is complete and conforms to a specified BagIt profile. Links: source , documentation , tests . workers/uploader.js uploads files to a remote upload target. Links: source , documentation , tests .","title":"The JobRunner, Step by Step"},{"location":"developers/logging/","text":"Logging DART uses the Winston library for logging. You can find the location of your DART logs by choosing Help > About from the main menu. You should see a dialog like this: As you develop, you can tail the logs to see what's happening in real time. Just substitute the proper log location in the command below. tail -f /Users/apd4n/Library/Logs/DART/dart.log Also note that you can tail the logs directly from DART by choose Help > Logs from the menu. While this approach is handy, there are two caveats. First, if the log file is large, the log window may take a long time to load. Second, DART rotates its logs when they reach a size of 5 MB. If that rollover happens while you're tailing the log in the DART log window, you won't see any entries logged after the rollover. DART logs to a different file when it runs unit tests, usually to a directory called .dart-test/log/dart.log . For more information on logging, see the code in util/logger.js and the config settings in core/config.js .","title":"Logging"},{"location":"developers/logging/#logging","text":"DART uses the Winston library for logging. You can find the location of your DART logs by choosing Help > About from the main menu. You should see a dialog like this: As you develop, you can tail the logs to see what's happening in real time. Just substitute the proper log location in the command below. tail -f /Users/apd4n/Library/Logs/DART/dart.log Also note that you can tail the logs directly from DART by choose Help > Logs from the menu. While this approach is handy, there are two caveats. First, if the log file is large, the log window may take a long time to load. Second, DART rotates its logs when they reach a size of 5 MB. If that rollover happens while you're tailing the log in the DART log window, you won't see any entries logged after the rollover. DART logs to a different file when it runs unit tests, usually to a directory called .dart-test/log/dart.log . For more information on logging, see the code in util/logger.js and the config settings in core/config.js .","title":"Logging"},{"location":"developers/testing/","text":"Testing DART DART uses Jest for testing. You can run DART's test suite with this command npm test -- --runInBand Note that the --runInBand option is required as a number of tests save fixture data to DART's disk-based data store. Because that data is globally accessible, tests must run in sequence to avoid deleting and overwriting data that other tests expect to be in a known state. Test Code You Add If you're adding code to DART, please write tests to ensure your code behaves as expected.","title":"Testing"},{"location":"developers/testing/#testing-dart","text":"DART uses Jest for testing. You can run DART's test suite with this command npm test -- --runInBand Note that the --runInBand option is required as a number of tests save fixture data to DART's disk-based data store. Because that data is globally accessible, tests must run in sequence to avoid deleting and overwriting data that other tests expect to be in a known state.","title":"Testing DART"},{"location":"developers/testing/#test-code-you-add","text":"If you're adding code to DART, please write tests to ensure your code behaves as expected.","title":"Test Code You Add"},{"location":"developers/plugins/","text":"Plugins Plugins allow DART to read and write data formats (such as tar, zip, etc.), to upload and download files using various protocols (such as s3, ftp, etc.), to communicate with the REST APIs of remote repositories (such as APTrust) and to help users through the complexities of initial setup and configuration (such as the APTrust and DPN setup modules). DART plugins are written in plain JavaScript and HTML. Developers may contribute new plugins without having to learn DART's internals. A plugin simply has to conform to a simple API, which it typically limited to a small number of well-defined methods. The documentation in this section gives a high-level overview plugin structure and behavior as a starting point for developing your own plugins. You'll find more detail information in DART's API documentation and source code, as well as in the unit tests that accompany existing plugins. Along with an overview of each plugin type, this documentation provides relevant links to source and test code for further study. The Base Plugin DART's base Plugin object is simply an EventEmitter with a static method that returns a description. The description includes the following fields: id - A UUID string that uniquely identifies the plugin. When writing a plugin, you should generate this once. It lives from then on as a hard-coded identifier. name - The name of the plugin. This will appear in parts of the DART UI that use the plugin. description - A description of what the plugin does. version - The plugin version, which is a string. E.g. '1.0.14'. readsFormats - A list of strings, this applies only to plugins that can read data from formats such as zip, tar, rar, parchive, etc. The values in this field should be the file extensions of the types of files the plugin can read. For example, ['.tar.', '.gz', '.gzip', '.tar.gz']. This should be empty for plugins that are not intended to read file formats. Use all lower case letters. writesFormats - A list of strings indicating the file formats the plugin can write. E.g. ['.tar.', '.gz', '.gzip', '.tar.gz']. This should be empty for plugins that are not intended to write file formats. Use all lower case letters. implementsProtocols - The network protocols that this plugin implements. For example, an FTP plugin may implement ['ftp', 'sftp', 'ftps']. This applies only to plugins of type NetworkClient. If your plugin is not a NetworkClient, this should be an empty list. Use all lower case letters. talksToRepository - This describes what type of repository your plugin talks to. For example, 'fedora', 'aptrust', etc. This applies only to plugins of type Repository. If your plugin is not a Repository plugin, this should be empty. Use all lower case letters. The PluginManager uses these descriptions to tell the application what plugins are available and what capabilities they have. Plugins are EventEmitters. They can work synchronously or asynchronously, but they must emit a standard set of events to communicate with the UI (or the JobRunner , when DART runs in command-line mode). Plugin Types Format Readers read file formats, such as tar, zip, and anything else a developer might want. Format Writers write file formats. Network Clients provide methods for uploading and download via different network protocols. Repo Clients provide communication between DART and the REST APIs of remote repositories. The PluginManager provides methods for discovering and loading installed plugins.","title":"Plugins"},{"location":"developers/plugins/#plugins","text":"Plugins allow DART to read and write data formats (such as tar, zip, etc.), to upload and download files using various protocols (such as s3, ftp, etc.), to communicate with the REST APIs of remote repositories (such as APTrust) and to help users through the complexities of initial setup and configuration (such as the APTrust and DPN setup modules). DART plugins are written in plain JavaScript and HTML. Developers may contribute new plugins without having to learn DART's internals. A plugin simply has to conform to a simple API, which it typically limited to a small number of well-defined methods. The documentation in this section gives a high-level overview plugin structure and behavior as a starting point for developing your own plugins. You'll find more detail information in DART's API documentation and source code, as well as in the unit tests that accompany existing plugins. Along with an overview of each plugin type, this documentation provides relevant links to source and test code for further study.","title":"Plugins"},{"location":"developers/plugins/#the-base-plugin","text":"DART's base Plugin object is simply an EventEmitter with a static method that returns a description. The description includes the following fields: id - A UUID string that uniquely identifies the plugin. When writing a plugin, you should generate this once. It lives from then on as a hard-coded identifier. name - The name of the plugin. This will appear in parts of the DART UI that use the plugin. description - A description of what the plugin does. version - The plugin version, which is a string. E.g. '1.0.14'. readsFormats - A list of strings, this applies only to plugins that can read data from formats such as zip, tar, rar, parchive, etc. The values in this field should be the file extensions of the types of files the plugin can read. For example, ['.tar.', '.gz', '.gzip', '.tar.gz']. This should be empty for plugins that are not intended to read file formats. Use all lower case letters. writesFormats - A list of strings indicating the file formats the plugin can write. E.g. ['.tar.', '.gz', '.gzip', '.tar.gz']. This should be empty for plugins that are not intended to write file formats. Use all lower case letters. implementsProtocols - The network protocols that this plugin implements. For example, an FTP plugin may implement ['ftp', 'sftp', 'ftps']. This applies only to plugins of type NetworkClient. If your plugin is not a NetworkClient, this should be an empty list. Use all lower case letters. talksToRepository - This describes what type of repository your plugin talks to. For example, 'fedora', 'aptrust', etc. This applies only to plugins of type Repository. If your plugin is not a Repository plugin, this should be empty. Use all lower case letters. The PluginManager uses these descriptions to tell the application what plugins are available and what capabilities they have. Plugins are EventEmitters. They can work synchronously or asynchronously, but they must emit a standard set of events to communicate with the UI (or the JobRunner , when DART runs in command-line mode).","title":"The Base Plugin"},{"location":"developers/plugins/#plugin-types","text":"Format Readers read file formats, such as tar, zip, and anything else a developer might want. Format Writers write file formats. Network Clients provide methods for uploading and download via different network protocols. Repo Clients provide communication between DART and the REST APIs of remote repositories. The PluginManager provides methods for discovering and loading installed plugins.","title":"Plugin Types"},{"location":"developers/plugins/format_readers/","text":"Format Readers Format readers read file formats like tar, zip, OCFL, etc. Format readers provide methods for listing the contents of a directory or serialized file, and for reading individual files from the directory/file. The initial release of DART 2.0 includes two format readers: a FileSystemReader and a TarReader . Developers can use these two examples as references for how to write a format reader. API and Events Format readers must extend the DART base Plugin and providing a description() method that returns meaningful PluginDescription information. Format readers must implement the following methods: A constructor that takes a single parameter, which is the path to the file that the reader will read. (If the reader reads from a directory, such as an OCFL object, the path should point to a directory.) A static definition method that takes no parameters and returns a description of the plugin (as described in The Base Plugin ). A read method that takes no parameters and emits events entry , error , and end (described below). A list method that takes no parameters and emits events entry , error , and end (described below). Property fileCount , a number that starts at zero and is incremented by one each time read or list emits an entry event for a file. Property dirCount , a number that starts at zero and is incremented by one each time read or list emits an entry event for a directory. Property byteCount , a number that starts at zero and is incremented by the number of bytes in a file each time read or list emits an entry event for a file. The error event passes a JavaScript error object to registered listeners and causes DART to stop reading. The end event passes nothing to registered listeners and tells DART that the reader has finished with all entries. The entry event passes an object containing the relative path of the current file entry, a lightweight FileStat object, and in the case a of the read method, an open Readable stream so DART can read the contents of the file. Note DART uses its own simplified FileStat object instead of Node's built-in Stats object because format readers will often have to read from serialized formats that don't include all of the information you'd find in a Stats object. For example, tar and zip headers may exclude information about inode, blocksize, etc. The FileStat object simply captures the essentials that are common across most formats. Tip In cases where a FormatReader's read method cannot return a readable stream, it can return a DummyReader . This is necessary in cases where the reader encounters a directory entry that cannot be read like a regular file. DART will still try to read from the readable stream in the entry event, and a DummyReader will allow DART to read without error. For an example of how to do this, look for DummyReader in the read method of the FileSystemReader source code. Format Reader Examples The best way to understand how to write a FormatReader plugin is to review the API documentation, source code, and tests for the following: FileSystemReader , which provides a working example. The FileSystemReader tests , which show how the reader is expected to behave. TarReader . The TarReader tests , which show how the TarReader is expected to behave. The FileStat documentation, which shows the structure of the FileStat object.","title":"Format Readers"},{"location":"developers/plugins/format_readers/#format-readers","text":"Format readers read file formats like tar, zip, OCFL, etc. Format readers provide methods for listing the contents of a directory or serialized file, and for reading individual files from the directory/file. The initial release of DART 2.0 includes two format readers: a FileSystemReader and a TarReader . Developers can use these two examples as references for how to write a format reader.","title":"Format Readers"},{"location":"developers/plugins/format_readers/#api-and-events","text":"Format readers must extend the DART base Plugin and providing a description() method that returns meaningful PluginDescription information. Format readers must implement the following methods: A constructor that takes a single parameter, which is the path to the file that the reader will read. (If the reader reads from a directory, such as an OCFL object, the path should point to a directory.) A static definition method that takes no parameters and returns a description of the plugin (as described in The Base Plugin ). A read method that takes no parameters and emits events entry , error , and end (described below). A list method that takes no parameters and emits events entry , error , and end (described below). Property fileCount , a number that starts at zero and is incremented by one each time read or list emits an entry event for a file. Property dirCount , a number that starts at zero and is incremented by one each time read or list emits an entry event for a directory. Property byteCount , a number that starts at zero and is incremented by the number of bytes in a file each time read or list emits an entry event for a file. The error event passes a JavaScript error object to registered listeners and causes DART to stop reading. The end event passes nothing to registered listeners and tells DART that the reader has finished with all entries. The entry event passes an object containing the relative path of the current file entry, a lightweight FileStat object, and in the case a of the read method, an open Readable stream so DART can read the contents of the file. Note DART uses its own simplified FileStat object instead of Node's built-in Stats object because format readers will often have to read from serialized formats that don't include all of the information you'd find in a Stats object. For example, tar and zip headers may exclude information about inode, blocksize, etc. The FileStat object simply captures the essentials that are common across most formats. Tip In cases where a FormatReader's read method cannot return a readable stream, it can return a DummyReader . This is necessary in cases where the reader encounters a directory entry that cannot be read like a regular file. DART will still try to read from the readable stream in the entry event, and a DummyReader will allow DART to read without error. For an example of how to do this, look for DummyReader in the read method of the FileSystemReader source code.","title":"API and Events"},{"location":"developers/plugins/format_readers/#format-reader-examples","text":"The best way to understand how to write a FormatReader plugin is to review the API documentation, source code, and tests for the following: FileSystemReader , which provides a working example. The FileSystemReader tests , which show how the reader is expected to behave. TarReader . The TarReader tests , which show how the TarReader is expected to behave. The FileStat documentation, which shows the structure of the FileStat object.","title":"Format Reader Examples"},{"location":"developers/plugins/format_writers/","text":"Format Writers Format writers write file formats like tar, zip, OCFL, etc. Format writers provide methods for writing contents into a directory or serialized file. The initial release of DART 2.0 includes two format writers: a FileSystemWriter and a TarWriter . Developers can use these two examples as references for how to write a format writer. Format writers may be synchronous or asynchronous under the hood. Certain types of writers, such as the built-in TarWriter MUST be synchronous internally because the tar format requires files to be written one at a time, in order. To assist with this, the BaseWriter implements a queue that executes requests sequentially, in the order they were received, with each write request beginning only after the last write has completed. API and Events Format writers should extend the DART BaseWriter and provide a description() method that returns meaningful PluginDescription information. Format writers must implement the following methods: A constructor that takes a single parameter, which is the path to the file or directory that the reader will write. A static definition method that takes no parameters and returns a description of the plugin (as described in The Base Plugin ). An add method that takes two parameters: a BagItFile and an options list of cryptographic hash algorithm names (such as 'md5', 'sha256', etc.). This method must emit a fileAdded event each time it writes a file into the target directory (or tar archive, zip archive, etc). Note that a BagItFile is a simple object with properties to denote the source path from which the file is copied, the destination path to which it should be copied, and some basic stats information that includes the file size. In the process of copying the file, the Format Writer should calculate the requested checksums and store them in the checksums hash of the BagItFile. Format writers should fire the fileAdded event each time a file has been written. See the add method of FileSystemWriter or TarWriter to see when and how this occurs. The data passed by the fileAdded event includes the BagItFile that was just added and the percent complete of the overall write operation (i.e. number of bytes written divided by number of total bytes to be written). Format writers won't need to emit the error event on their own, unless some special circumstance warrents it. The BaseWriter emits this event, passing back a string showing the name of the file being written and the error message. Note The BaseWriter always emits the finish event immediately after emitting the error event. This is done on the assumption that there is no sense in continuing to write a failed, incomplete, or corrupt output file. Format writers don't need to emit the finish event. The BaseWriter takes care of that internally. This event does not pass any parameters. Note After emission of the finish event, you should be able to check the filesWritten property to get the number of files written. That number is updated by the BaseWriter's onFileWritten method. If you override that method, be sure your override calls super() . Format Writer Queue Functions DART's BaseWriter uses a one-at-a-time queue to force sequential synchronous writes. The queue is an instance of async.queue , and it requires a function to be run on each item. The writeIntoArchive() functions in FileSystemWriter source and TarWriter source provide examples on how to write a function for the queue. Notice that in both format writers, the add() method sets up a hash containing data and functions, then passes that hash into the queue by calling this._queue.push(data) . The queue eventually hands that data structure off to writeIntoArchive() , which does some piping internally to calculate multiple checksums in a single write.","title":"Format Writers"},{"location":"developers/plugins/format_writers/#format-writers","text":"Format writers write file formats like tar, zip, OCFL, etc. Format writers provide methods for writing contents into a directory or serialized file. The initial release of DART 2.0 includes two format writers: a FileSystemWriter and a TarWriter . Developers can use these two examples as references for how to write a format writer. Format writers may be synchronous or asynchronous under the hood. Certain types of writers, such as the built-in TarWriter MUST be synchronous internally because the tar format requires files to be written one at a time, in order. To assist with this, the BaseWriter implements a queue that executes requests sequentially, in the order they were received, with each write request beginning only after the last write has completed.","title":"Format Writers"},{"location":"developers/plugins/format_writers/#api-and-events","text":"Format writers should extend the DART BaseWriter and provide a description() method that returns meaningful PluginDescription information. Format writers must implement the following methods: A constructor that takes a single parameter, which is the path to the file or directory that the reader will write. A static definition method that takes no parameters and returns a description of the plugin (as described in The Base Plugin ). An add method that takes two parameters: a BagItFile and an options list of cryptographic hash algorithm names (such as 'md5', 'sha256', etc.). This method must emit a fileAdded event each time it writes a file into the target directory (or tar archive, zip archive, etc). Note that a BagItFile is a simple object with properties to denote the source path from which the file is copied, the destination path to which it should be copied, and some basic stats information that includes the file size. In the process of copying the file, the Format Writer should calculate the requested checksums and store them in the checksums hash of the BagItFile. Format writers should fire the fileAdded event each time a file has been written. See the add method of FileSystemWriter or TarWriter to see when and how this occurs. The data passed by the fileAdded event includes the BagItFile that was just added and the percent complete of the overall write operation (i.e. number of bytes written divided by number of total bytes to be written). Format writers won't need to emit the error event on their own, unless some special circumstance warrents it. The BaseWriter emits this event, passing back a string showing the name of the file being written and the error message. Note The BaseWriter always emits the finish event immediately after emitting the error event. This is done on the assumption that there is no sense in continuing to write a failed, incomplete, or corrupt output file. Format writers don't need to emit the finish event. The BaseWriter takes care of that internally. This event does not pass any parameters. Note After emission of the finish event, you should be able to check the filesWritten property to get the number of files written. That number is updated by the BaseWriter's onFileWritten method. If you override that method, be sure your override calls super() .","title":"API and Events"},{"location":"developers/plugins/format_writers/#format-writer-queue-functions","text":"DART's BaseWriter uses a one-at-a-time queue to force sequential synchronous writes. The queue is an instance of async.queue , and it requires a function to be run on each item. The writeIntoArchive() functions in FileSystemWriter source and TarWriter source provide examples on how to write a function for the queue. Notice that in both format writers, the add() method sets up a hash containing data and functions, then passes that hash into the queue by calling this._queue.push(data) . The queue eventually hands that data structure off to writeIntoArchive() , which does some piping internally to calculate multiple checksums in a single write.","title":"Format Writer Queue Functions"},{"location":"developers/plugins/manager/","text":"Plugin Manager DART's PluginManager provides methods that allow the application to discover and load plugins. DART uses the findById method to load individual plugins, and the following methods to discover available plugins: getModuleCollection() - This returns information about plugins of a specified type, such as Repo Clients or Network Clients . The list of plugins returned by this method appears on some configuration screens. For example, when a user sets up a new StorageService , a list of available network clients appears so the user can choose which client/protocol should be used to communicate with that service. DART's JobRunner uses the methods canRead(), canWrite(), implementsProtocol(), talksTo(), and setsUp() to figure out which plugins to use to complete each operation within a job. See also: PluginManager API , PluginManager source","title":"Plugin Manager"},{"location":"developers/plugins/manager/#plugin-manager","text":"DART's PluginManager provides methods that allow the application to discover and load plugins. DART uses the findById method to load individual plugins, and the following methods to discover available plugins: getModuleCollection() - This returns information about plugins of a specified type, such as Repo Clients or Network Clients . The list of plugins returned by this method appears on some configuration screens. For example, when a user sets up a new StorageService , a list of available network clients appears so the user can choose which client/protocol should be used to communicate with that service. DART's JobRunner uses the methods canRead(), canWrite(), implementsProtocol(), talksTo(), and setsUp() to figure out which plugins to use to complete each operation within a job. See also: PluginManager API , PluginManager source","title":"Plugin Manager"},{"location":"developers/plugins/network_clients/","text":"Network Clients Network Clients allow DART to send and receive data using defined network protocols such as S3, FTP, etc. These clients should be able to do the following: Use the information in a StorageService object to login to or authenticate with the remote service. StorageService objects include a URL, login name, password, and other information required for authentication. Upload files or data to the remote service (if the service supports upload). Download files or data from the remote service (if the service supports download). List files or objects in the remote service (if the service supports list operations). Note DART 2.0.16 supports only S3 and SFTP uploads. API and Events Network clients must implement the following: A constructor that takes a StorageService object as its sole parameter. An upload method that takes two parameters, localPath and remotePath . This method uploads the file at localPath to the destination remotePath. A download method that takes two parameters, localPath and remotePath . This method downloads the file from remotePath to localPath. A list method whose parameters and return values are yet to be determined. Warning The API for network client plugins is still in flux. We may add optional parameters to each of the methods above, and we may further define some as-yet undefined behaviors. We plan to flesh out the network client API as we add new clients. Network clients emit the following events: start - Indicates an upload or download has started. status - Indicates the status of an upload or download operation. warning - Issued when a non-fatal error occurs, such as a network connection interruption. The client should retry operations that failed due to non-fatal errors. error - Issued when a fatal error occurs. Fatal errors include authentication failures, missing files, permission errors, and other problems you cannot reasonably expect to go away when you retry the operation. finish - Emitted when the upload/download is complete. The Transfer Object Network clients may internally use their own custom transfer object to keep track of the details of an upload or download operation. The structure of a custom transfer object is entirely up to the developer. The only requirement is that the object include a property called result of type OperationResult . DART doesn't care what network client's do under the hood, but it does care about the results of each operation. Those results should be stored in the OperationResult object, and that object should be made accessible to DART through the events listed above. Reference Implementation The only current reference implementation for a network client plugin is the S3Client . The S3Client tests will give you an idea of how the client is expected to behave.","title":"Network Clients"},{"location":"developers/plugins/network_clients/#network-clients","text":"Network Clients allow DART to send and receive data using defined network protocols such as S3, FTP, etc. These clients should be able to do the following: Use the information in a StorageService object to login to or authenticate with the remote service. StorageService objects include a URL, login name, password, and other information required for authentication. Upload files or data to the remote service (if the service supports upload). Download files or data from the remote service (if the service supports download). List files or objects in the remote service (if the service supports list operations). Note DART 2.0.16 supports only S3 and SFTP uploads.","title":"Network Clients"},{"location":"developers/plugins/network_clients/#api-and-events","text":"Network clients must implement the following: A constructor that takes a StorageService object as its sole parameter. An upload method that takes two parameters, localPath and remotePath . This method uploads the file at localPath to the destination remotePath. A download method that takes two parameters, localPath and remotePath . This method downloads the file from remotePath to localPath. A list method whose parameters and return values are yet to be determined. Warning The API for network client plugins is still in flux. We may add optional parameters to each of the methods above, and we may further define some as-yet undefined behaviors. We plan to flesh out the network client API as we add new clients. Network clients emit the following events: start - Indicates an upload or download has started. status - Indicates the status of an upload or download operation. warning - Issued when a non-fatal error occurs, such as a network connection interruption. The client should retry operations that failed due to non-fatal errors. error - Issued when a fatal error occurs. Fatal errors include authentication failures, missing files, permission errors, and other problems you cannot reasonably expect to go away when you retry the operation. finish - Emitted when the upload/download is complete.","title":"API and Events"},{"location":"developers/plugins/network_clients/#the-transfer-object","text":"Network clients may internally use their own custom transfer object to keep track of the details of an upload or download operation. The structure of a custom transfer object is entirely up to the developer. The only requirement is that the object include a property called result of type OperationResult . DART doesn't care what network client's do under the hood, but it does care about the results of each operation. Those results should be stored in the OperationResult object, and that object should be made accessible to DART through the events listed above.","title":"The Transfer Object"},{"location":"developers/plugins/network_clients/#reference-implementation","text":"The only current reference implementation for a network client plugin is the S3Client . The S3Client tests will give you an idea of how the client is expected to behave.","title":"Reference Implementation"},{"location":"developers/plugins/repo_clients/","text":"Repository Clients Repository clients communicate with the REST APIs of remote repositories such as APTrust, Fedora, etc. For safety, these clients should be limited to read operations, such as showing a list of ingested objects or a list of pending work items from a queue. API Repository clients should inherit from RepositoryBase . They must implement the following methods: A constructor that takes a RemoteRepository object as its first parameter. A provides method that takes no parameters and returns a list of object describing what methods this object provides. Each of the described methods should return a promise that ultimately returns HTML to be displayed in the DART dashboard. See the APTrustClient for an example. A hasRequiredConnectionInfo method that takes no parameters and returns true or false to indicate whether the client's RemoteRepository object appears to have enough info to query the repository. If this returns false, the DART dashboard will skip its automatic attempt to connect to the repo. Report Templates Because repository clients return HTML, they may use templates to translate the repository's JSON responses to fomatted markup. These templates should be in a directory whose name matches the file name of repository client, minus the .js at the end. For example, the APTrustClient is in the file plugins/repository/aptrust.js. It's templates are in plugins/repository/aptrust/. DART uses Handlebars templates. The APTrustClient includes templates to display ingested objects and work items , which are items queued for ingest or restoration. Since the APTrust client is currently the only existing repository client plugin, its source code is probably the best reference for writing your own client. The APTrustClient tests show how the client is expected to behave. You can see what the output of the APTrust client looks like in the dashboard documentation . If you're interested in seeing how the dashboard loads reports from repository clients, see the show() method of the DashboardController . Warning The API for repository clients is subject to change. Most changes will likely be additions.","title":"Repository Clients"},{"location":"developers/plugins/repo_clients/#repository-clients","text":"Repository clients communicate with the REST APIs of remote repositories such as APTrust, Fedora, etc. For safety, these clients should be limited to read operations, such as showing a list of ingested objects or a list of pending work items from a queue.","title":"Repository Clients"},{"location":"developers/plugins/repo_clients/#api","text":"Repository clients should inherit from RepositoryBase . They must implement the following methods: A constructor that takes a RemoteRepository object as its first parameter. A provides method that takes no parameters and returns a list of object describing what methods this object provides. Each of the described methods should return a promise that ultimately returns HTML to be displayed in the DART dashboard. See the APTrustClient for an example. A hasRequiredConnectionInfo method that takes no parameters and returns true or false to indicate whether the client's RemoteRepository object appears to have enough info to query the repository. If this returns false, the DART dashboard will skip its automatic attempt to connect to the repo.","title":"API"},{"location":"developers/plugins/repo_clients/#report-templates","text":"Because repository clients return HTML, they may use templates to translate the repository's JSON responses to fomatted markup. These templates should be in a directory whose name matches the file name of repository client, minus the .js at the end. For example, the APTrustClient is in the file plugins/repository/aptrust.js. It's templates are in plugins/repository/aptrust/. DART uses Handlebars templates. The APTrustClient includes templates to display ingested objects and work items , which are items queued for ingest or restoration. Since the APTrust client is currently the only existing repository client plugin, its source code is probably the best reference for writing your own client. The APTrustClient tests show how the client is expected to behave. You can see what the output of the APTrust client looks like in the dashboard documentation . If you're interested in seeing how the dashboard loads reports from repository clients, see the show() method of the DashboardController . Warning The API for repository clients is subject to change. Most changes will likely be additions.","title":"Report Templates"},{"location":"shared/downloads/","text":"Updated October 27, 2021 Download the DART installer for your system. The current version is 2.0.16 (released October 27, 2021). Mac : https://s3.amazonaws.com/aptrust.public.download/DART/DART-2.0.16.dmg Windows : https://s3.amazonaws.com/aptrust.public.download/DART/DART+Setup+2.0.16.exe Linux : https://s3.amazonaws.com/aptrust.public.download/DART/DART_2.0.16_amd64.deb Double-click the installer after download and follow the prompts on screen.","title":"Downloads"},{"location":"users/command_line/","text":"Command Line Reference DART provides several ways of running jobs from the command line. The most convenient method, and the easiest to script, is to define a workflow and then create a JobParams object that describes which files to pass through the workflow and what custom metadata attributes to assign. Note that in all of these examples, the dart command is followed by two dashes, and then the command-line parameters. Parameters to the left of the double dash will be consumed by the Node.js runtime, while those to the right will be passed to DART. Note Replace dart in the examples below with the full path to DART on your computer. On Windows, the default path to the DART executable is C:\\Users\\<USERNAME>\\AppData\\Local\\Programs\\DART\\DART.exe . Replace <USERNAME> with your user name. On Mac, the default path to DART is /Applications/DART.app/Contents/MacOS/DART . From a JobParams JSON file dart -- --job path/to/job_params.json From JobParams JSON passed through STDIN echo \"{ ... json ... }\" | dart -- --stdin From a Job JSON file dart -- --job path/to/job.json Note: DART does not yet export jobs to JSON. That feature is in the works. From Job JSON passed through STDIN echo { ... json ... } | dart -- --stdin Note Job JSON can be relatively large, since it may include a BagIt profile. Using JobParams is generally easier than using Jobs. By Job UUID dart -- --job 00000000-0000-0000-0000-000000000000 Note The ability to extract Job UUIDs from DART is coming soon. As of this writing (July 22, 2019), some elements of the CLI are subject to change and further development. Resource Usage and Known Issues DART uses Node.js, which is a relatively fast, efficient JavaScript runtime. Node, however, can use large amounts of memory, often 10-30 times what a similar application written in Go or C++ may use. You can expect each DART command-line process to use at least 50 MB of RAM on startup, and sometimes over 150 MB at runtime if it when creating large packages. Potentially High Memory Usage During S3 Uploads DART consumes the most memory when uploading large files to an S3-compliant endpoint. S3 clients need to load chunks of data into memory before streaming them across the network. When uploading a file of 100 MB, each chunk may be only 5-50 MB in size, increasing DART's memory usage by that amound until the chunk has been copied to the remote server. S3 allows uploads of up to 5 TB, with a maximum of 10,000 chunks. This means that when uploading a 5 TB file, DART's underlying S3 client will load chunks of about 535 MB each into memory. That's a considerable amount of memory, particularly on a computer that likely has a number of other open appilcations. For this reason, it's best to avoid running multiple simultaneous multi-terabyte jobs. Disk Usage When Creating Large Packages When you're packaging very large files, keep in mind that you may run out of disk space. For example, DART needs about 5 TB of disk space to create a 5 TB bag. DART is fairly efficient about this, writing contents directly into a tar archive when possible to avoid multiple copies, and calculating all checksums in a single pass during the writes. However, if you're creating a 5 TB bag and you don't have 5 TB of disk space, the job is going to fail. Note that you can change the location of your bagging directory by choosing Settings > App Settings from the menu and then clicking Bagging Directory . You can also change your bagging directory for a single job by setting the Output Path for that job. If you need extra disk space for a particular job, consider using a network share or an external USB drive as your bagging directory. Checking the Logs for Failed Jobs DART logs all of its work. If a job fails, check the logs for details. Sharing Jobs Note that because details of jobs, workflows, and storage services are stored on your local DART computer, workflows and JobParams defined on one machine will not run on another machine that does not have matching settings.","title":"Command Line Reference"},{"location":"users/command_line/#command-line-reference","text":"DART provides several ways of running jobs from the command line. The most convenient method, and the easiest to script, is to define a workflow and then create a JobParams object that describes which files to pass through the workflow and what custom metadata attributes to assign. Note that in all of these examples, the dart command is followed by two dashes, and then the command-line parameters. Parameters to the left of the double dash will be consumed by the Node.js runtime, while those to the right will be passed to DART. Note Replace dart in the examples below with the full path to DART on your computer. On Windows, the default path to the DART executable is C:\\Users\\<USERNAME>\\AppData\\Local\\Programs\\DART\\DART.exe . Replace <USERNAME> with your user name. On Mac, the default path to DART is /Applications/DART.app/Contents/MacOS/DART . From a JobParams JSON file dart -- --job path/to/job_params.json From JobParams JSON passed through STDIN echo \"{ ... json ... }\" | dart -- --stdin From a Job JSON file dart -- --job path/to/job.json Note: DART does not yet export jobs to JSON. That feature is in the works. From Job JSON passed through STDIN echo { ... json ... } | dart -- --stdin Note Job JSON can be relatively large, since it may include a BagIt profile. Using JobParams is generally easier than using Jobs. By Job UUID dart -- --job 00000000-0000-0000-0000-000000000000 Note The ability to extract Job UUIDs from DART is coming soon. As of this writing (July 22, 2019), some elements of the CLI are subject to change and further development.","title":"Command Line Reference"},{"location":"users/command_line/#resource-usage-and-known-issues","text":"DART uses Node.js, which is a relatively fast, efficient JavaScript runtime. Node, however, can use large amounts of memory, often 10-30 times what a similar application written in Go or C++ may use. You can expect each DART command-line process to use at least 50 MB of RAM on startup, and sometimes over 150 MB at runtime if it when creating large packages.","title":"Resource Usage and Known Issues"},{"location":"users/command_line/#potentially-high-memory-usage-during-s3-uploads","text":"DART consumes the most memory when uploading large files to an S3-compliant endpoint. S3 clients need to load chunks of data into memory before streaming them across the network. When uploading a file of 100 MB, each chunk may be only 5-50 MB in size, increasing DART's memory usage by that amound until the chunk has been copied to the remote server. S3 allows uploads of up to 5 TB, with a maximum of 10,000 chunks. This means that when uploading a 5 TB file, DART's underlying S3 client will load chunks of about 535 MB each into memory. That's a considerable amount of memory, particularly on a computer that likely has a number of other open appilcations. For this reason, it's best to avoid running multiple simultaneous multi-terabyte jobs.","title":"Potentially High Memory Usage During S3 Uploads"},{"location":"users/command_line/#disk-usage-when-creating-large-packages","text":"When you're packaging very large files, keep in mind that you may run out of disk space. For example, DART needs about 5 TB of disk space to create a 5 TB bag. DART is fairly efficient about this, writing contents directly into a tar archive when possible to avoid multiple copies, and calculating all checksums in a single pass during the writes. However, if you're creating a 5 TB bag and you don't have 5 TB of disk space, the job is going to fail. Note that you can change the location of your bagging directory by choosing Settings > App Settings from the menu and then clicking Bagging Directory . You can also change your bagging directory for a single job by setting the Output Path for that job. If you need extra disk space for a particular job, consider using a network share or an external USB drive as your bagging directory.","title":"Disk Usage When Creating Large Packages"},{"location":"users/command_line/#checking-the-logs-for-failed-jobs","text":"DART logs all of its work. If a job fails, check the logs for details.","title":"Checking the Logs for Failed Jobs"},{"location":"users/command_line/#sharing-jobs","text":"Note that because details of jobs, workflows, and storage services are stored on your local DART computer, workflows and JobParams defined on one machine will not run on another machine that does not have matching settings.","title":"Sharing Jobs"},{"location":"users/dart_runner/","text":"DART Runner Because DART uses the Electron framework, it requires the presence of a graphical user interface and a windowing system, even when it's not going to use a GUI. That means it can't even start in comman-line mode unless it's running in a desktop environment. This limitation is inherent in Electron and makes DART unsuitable for running on a Linux server. For this reason, APTrust built dart-runner, which is a lightweight command-line version of DART that can run in server environments without a GUI. Dart-runner is intended to run workflows that were created and tested in DART. The general process is: Create a Workflow in DART and test it out locally to ensure it does what you want. Export the workflow as described on the Workflows page . Run dart-runner on the server with the exported workflow file and a list of items you want to run through that workflow. The list must conform to the workflow CSV format used for batch jobs . Note When scripting jobs and workflows on Mac and Windows, you should stick with the DART CLI, since it's more mature as of late 2021. Since Windows and Mac include a GUI environment by default, DART and the DART CLI will always work. Features Dart-runner: is a single, lightweight binary, less than 10MB in size, with no external dependencies. To install, just copy the binary to your server. is much lighter on CPU and memory than DART. can run huge workflows unattended. can run multiple jobs in parallel. can be scripted from other languages like the DART CLI, though its syntax differs somewhat from the CLI. outputs clean, machine-readable JSON describing the outcome of every job. The output format is JSON Lines , with each line describing the outcome of a single job in the workflow. provides meaningful error messages when errors occur. supports piping, so you can send its output to any file you want for later analysis. provides meaningful return codes, so your script knows whether a job succeeded or failed. Limitations Dart-runner is currently in beta and has the following limitations: It supports only the BagIt packaging format. It supports only S3 uploads (no SFTP). It's intended primarily for use on Linux servers. Getting Started Download the initial 0.9 beta version of dart-runner for Linux . There's also a Mac version of the beta if you want to experiment, but for now, APTrust suggests using the DART CLI on Mac. Because it's a single binary with no dependencies, there's no installation process for dart-runner. Simply copy the binary onto your computer and run. If you're interested, the souce code is available on GitHub at https://github.com/APTrust/dart-runner . To view the built-in docs, run dart-runner --help , the contents of which appear below. DART Runner : Bag and ship files from the command line . To use DART Runner , you typically want to define a job or workflow in the DART UI , then export it as a json file to be consumed by DART Runner . See the Resources section below . ------- Options ------- -- job Path to job json file . Use this option only if you are running a single job ( as opposed to a workflow ) . Job json files can be exported from the DART UI . -- workflow Path to workflow json file . Use this option if you are running a workflow against a batch of files . If you specify a workflow file , you must also specify -- batch . Workflows can be exported from the DART UI . -- batch Path to CSV batch file . Use this option with -- workflow to specify a set of files or directories to run through a workflow . -- output - dir Path to package output directory . Jobs and workflows will create bags in this directory . This option is always REQUIRED . -- delete Delete bags after job completes ? Set this to true or false . The default is true for jobs and workflows that include uploads : the bags will be deleted after successful uploads . Default is false for jobs and workflows that do not include uploads because you probably want to do something with the bag after it 's created. -- concurrency Number of jobs to run concurrently . Default is 1. Max value for this param should be less than or equal to the number of processors on your machine . You may get diminishing returns when setting this above 2 because most of the DART runner 's work is reading from and writing to disk . -- help Show this help document . -------- Examples -------- To run a single job : dart - runner -- job = path / to / job . json -- output - dir = path / to / output This runs the job described in the job . json file , writing the bag to the specified output directory . To run a workflow : dart - runner -- workflow = path / to / workflow . json \\ -- batch = path / to / batch . csv \\ -- output - dir = path / to / directory \\ -- concurrency = 2 \\ -- delete = false The command above runs all of the items listed in the -- batch CSV file through the workflow described in the -- workflow json file . Bags are written to the output directory . Setting the delete flag to false means the bags will not be deleted from the output directory after successful upload . The -- concurrency flag above tells DART runner to work on 2 bags at a time ( instead of the default 1 at a time ) when bagging and uploading . Setting -- delete to true ( or omitting -- delete ) will cause bags to be deleted after successful upload . --------- Resources --------- DART Source : https : // github . com / APTrust / dart User Guide : https : // aptrust . github . io / dart - docs / DART Runner Source : https : // github . com / APTrust / dart - runner User Guide : https : // aptrust . github . io / dart - docs / users / dart - runner / DART and DART Runner are free and open source projects from APTrust . org .","title":"Dart Runner"},{"location":"users/dart_runner/#dart-runner","text":"Because DART uses the Electron framework, it requires the presence of a graphical user interface and a windowing system, even when it's not going to use a GUI. That means it can't even start in comman-line mode unless it's running in a desktop environment. This limitation is inherent in Electron and makes DART unsuitable for running on a Linux server. For this reason, APTrust built dart-runner, which is a lightweight command-line version of DART that can run in server environments without a GUI. Dart-runner is intended to run workflows that were created and tested in DART. The general process is: Create a Workflow in DART and test it out locally to ensure it does what you want. Export the workflow as described on the Workflows page . Run dart-runner on the server with the exported workflow file and a list of items you want to run through that workflow. The list must conform to the workflow CSV format used for batch jobs . Note When scripting jobs and workflows on Mac and Windows, you should stick with the DART CLI, since it's more mature as of late 2021. Since Windows and Mac include a GUI environment by default, DART and the DART CLI will always work.","title":"DART Runner"},{"location":"users/dart_runner/#features","text":"Dart-runner: is a single, lightweight binary, less than 10MB in size, with no external dependencies. To install, just copy the binary to your server. is much lighter on CPU and memory than DART. can run huge workflows unattended. can run multiple jobs in parallel. can be scripted from other languages like the DART CLI, though its syntax differs somewhat from the CLI. outputs clean, machine-readable JSON describing the outcome of every job. The output format is JSON Lines , with each line describing the outcome of a single job in the workflow. provides meaningful error messages when errors occur. supports piping, so you can send its output to any file you want for later analysis. provides meaningful return codes, so your script knows whether a job succeeded or failed.","title":"Features"},{"location":"users/dart_runner/#limitations","text":"Dart-runner is currently in beta and has the following limitations: It supports only the BagIt packaging format. It supports only S3 uploads (no SFTP). It's intended primarily for use on Linux servers.","title":"Limitations"},{"location":"users/dart_runner/#getting-started","text":"Download the initial 0.9 beta version of dart-runner for Linux . There's also a Mac version of the beta if you want to experiment, but for now, APTrust suggests using the DART CLI on Mac. Because it's a single binary with no dependencies, there's no installation process for dart-runner. Simply copy the binary onto your computer and run. If you're interested, the souce code is available on GitHub at https://github.com/APTrust/dart-runner . To view the built-in docs, run dart-runner --help , the contents of which appear below. DART Runner : Bag and ship files from the command line . To use DART Runner , you typically want to define a job or workflow in the DART UI , then export it as a json file to be consumed by DART Runner . See the Resources section below . ------- Options ------- -- job Path to job json file . Use this option only if you are running a single job ( as opposed to a workflow ) . Job json files can be exported from the DART UI . -- workflow Path to workflow json file . Use this option if you are running a workflow against a batch of files . If you specify a workflow file , you must also specify -- batch . Workflows can be exported from the DART UI . -- batch Path to CSV batch file . Use this option with -- workflow to specify a set of files or directories to run through a workflow . -- output - dir Path to package output directory . Jobs and workflows will create bags in this directory . This option is always REQUIRED . -- delete Delete bags after job completes ? Set this to true or false . The default is true for jobs and workflows that include uploads : the bags will be deleted after successful uploads . Default is false for jobs and workflows that do not include uploads because you probably want to do something with the bag after it 's created. -- concurrency Number of jobs to run concurrently . Default is 1. Max value for this param should be less than or equal to the number of processors on your machine . You may get diminishing returns when setting this above 2 because most of the DART runner 's work is reading from and writing to disk . -- help Show this help document . -------- Examples -------- To run a single job : dart - runner -- job = path / to / job . json -- output - dir = path / to / output This runs the job described in the job . json file , writing the bag to the specified output directory . To run a workflow : dart - runner -- workflow = path / to / workflow . json \\ -- batch = path / to / batch . csv \\ -- output - dir = path / to / directory \\ -- concurrency = 2 \\ -- delete = false The command above runs all of the items listed in the -- batch CSV file through the workflow described in the -- workflow json file . Bags are written to the output directory . Setting the delete flag to false means the bags will not be deleted from the output directory after successful upload . The -- concurrency flag above tells DART runner to work on 2 bags at a time ( instead of the default 1 at a time ) when bagging and uploading . Setting -- delete to true ( or omitting -- delete ) will cause bags to be deleted after successful upload . --------- Resources --------- DART Source : https : // github . com / APTrust / dart User Guide : https : // aptrust . github . io / dart - docs / DART Runner Source : https : // github . com / APTrust / dart - runner User Guide : https : // aptrust . github . io / dart - docs / users / dart - runner / DART and DART Runner are free and open source projects from APTrust . org .","title":"Getting Started"},{"location":"users/dashboard/","text":"Dashboard The dashboard shows running jobs, recently completed jobs, and selected items from remote repositories. Running Jobs The Running Jobs panel shows jobs DART is currently running on your computer. If more than one job is currently running, you can scroll inside the panel to see the progress of each. Also note the blue badge in the upper right corner of the window showing there are two running jobs. The blue badge appears on all DART views as long as jobs are running. Note that DART runs each job in a separate process. Actions you take in DART do not affect the running jobs. Warning When you shut down the DART application, all DART jobs stop, even if they are not yet complete. Don't close the DART window while jobs are running, unless you intend to stop all of the jobs. Progress Bars for Running Jobs The progress bars for running jobs show how much progress DART has made in each of the job's steps, which may include packaging, validation, and/or uploading. Info While the progress bars for packaging and validation are very accurate, the progress bar for uploads runs slightly ahead of the actual upload progress. DART knows how many bytes of an upload it has prepared to send, but not how many have been received by the remote host. It's common for the upload progress bar to appear to stall at about 98% while the last chunk of data goes across the wire. For smaller uploads, the stall may last only a second or two. For very large uploads, the final chunk may be hundreds of megabytes and may take several minutes to complete. Recent Jobs The Recent Jobs panel lists recently completed jobs. The Outcome column shows the job's last completed step, while the Date column shows when that step was completed. You can get more detailed information by clicking Jobs > List from the top menu. See also: Jobs Repositories The Repositories panels show items from remote repositories that DART knows how to connect to. In the screenshot above, this panel shows items recently ingested into APTrust's demo repository: The panel below shows a list of pending or recently completed tasks from APTrust demo system. Some repository panels, such as those from APTrust, show additional information when you mouse over an item. The panels show errors if they cannot communicate with the remote repository. If you run into errors like this, chances are your Remote Repository is incorrectly configured. Info Remote repository panels require both a correctly configured Remote Repository setting and a plugin that knows how to communicate with the remote repository. Plugins are typically written by developers associated with the repository, and are packaged with the DART installation. See also: Remote Repository","title":"Dashboard"},{"location":"users/dashboard/#dashboard","text":"The dashboard shows running jobs, recently completed jobs, and selected items from remote repositories.","title":"Dashboard"},{"location":"users/dashboard/#running-jobs","text":"The Running Jobs panel shows jobs DART is currently running on your computer. If more than one job is currently running, you can scroll inside the panel to see the progress of each. Also note the blue badge in the upper right corner of the window showing there are two running jobs. The blue badge appears on all DART views as long as jobs are running. Note that DART runs each job in a separate process. Actions you take in DART do not affect the running jobs. Warning When you shut down the DART application, all DART jobs stop, even if they are not yet complete. Don't close the DART window while jobs are running, unless you intend to stop all of the jobs.","title":"Running Jobs"},{"location":"users/dashboard/#progress-bars-for-running-jobs","text":"The progress bars for running jobs show how much progress DART has made in each of the job's steps, which may include packaging, validation, and/or uploading. Info While the progress bars for packaging and validation are very accurate, the progress bar for uploads runs slightly ahead of the actual upload progress. DART knows how many bytes of an upload it has prepared to send, but not how many have been received by the remote host. It's common for the upload progress bar to appear to stall at about 98% while the last chunk of data goes across the wire. For smaller uploads, the stall may last only a second or two. For very large uploads, the final chunk may be hundreds of megabytes and may take several minutes to complete.","title":"Progress Bars for Running Jobs"},{"location":"users/dashboard/#recent-jobs","text":"The Recent Jobs panel lists recently completed jobs. The Outcome column shows the job's last completed step, while the Date column shows when that step was completed. You can get more detailed information by clicking Jobs > List from the top menu. See also: Jobs","title":"Recent Jobs"},{"location":"users/dashboard/#repositories","text":"The Repositories panels show items from remote repositories that DART knows how to connect to. In the screenshot above, this panel shows items recently ingested into APTrust's demo repository: The panel below shows a list of pending or recently completed tasks from APTrust demo system. Some repository panels, such as those from APTrust, show additional information when you mouse over an item. The panels show errors if they cannot communicate with the remote repository. If you run into errors like this, chances are your Remote Repository is incorrectly configured. Info Remote repository panels require both a correctly configured Remote Repository setting and a plugin that knows how to communicate with the remote repository. Plugins are typically written by developers associated with the repository, and are packaged with the DART installation. See also: Remote Repository","title":"Repositories"},{"location":"users/faq/","text":"Frequently Asked Questions What is DART's bag size limit? DART can create bags of almost any size, provided you have enough disk space. Several APTrust depositors have used DART to create and upload bags in the 500 GB - 800 GB range. The largest so far has been 1.5 TB. Things to look out for when creating large bags Be sure you have enough disk space to actually bag the files. In some cases, may need to reset the output path for a large bag, to point to an external drive. See the Output Path setting in packaging for details. If you're bagging files from a shared network drive with a spotty connection, the bagging operation may fail if the network connection is interrupted. In these cases, you may have more success creating ten 10 GB bags than one 100 GB bag. DART validates bags after it creates them, and can take hours to validate very large bags. DART spends most of this time validating file checksums against the manifests. Be patient! Uploading large bags to an S3 or SFTP server can take a long time, especially if your network connection is slow or unreliable. The maximum size for S3 objects is 5 TB. If you create a bag larger than that, you will not be able to upload it to S3. SFTP file size limits are set individually for each system by the administrator.","title":"FAQ"},{"location":"users/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"users/faq/#what-is-darts-bag-size-limit","text":"DART can create bags of almost any size, provided you have enough disk space. Several APTrust depositors have used DART to create and upload bags in the 500 GB - 800 GB range. The largest so far has been 1.5 TB.","title":"What is DART's bag size limit?"},{"location":"users/faq/#things-to-look-out-for-when-creating-large-bags","text":"Be sure you have enough disk space to actually bag the files. In some cases, may need to reset the output path for a large bag, to point to an external drive. See the Output Path setting in packaging for details. If you're bagging files from a shared network drive with a spotty connection, the bagging operation may fail if the network connection is interrupted. In these cases, you may have more success creating ten 10 GB bags than one 100 GB bag. DART validates bags after it creates them, and can take hours to validate very large bags. DART spends most of this time validating file checksums against the manifests. Be patient! Uploading large bags to an S3 or SFTP server can take a long time, especially if your network connection is slow or unreliable. The maximum size for S3 objects is 5 TB. If you create a bag larger than that, you will not be able to upload it to S3. SFTP file size limits are set individually for each system by the administrator.","title":"Things to look out for when creating large bags"},{"location":"users/getting_started/","text":"Getting Started DART is the Digital Archivist's Resource Tool. Its primary purpose is to package digital materials and send them off to long-term preservation storage. DART's initial release focuses on packaging materials in BagIt format and uploading them to S3 buckets for ingest in APTrust. DART can be extended through plugins to produce other packaging formats and to communicated via additional network protocols. DART runs in both graphical and command-line modes on Windows, Mac, and Linux. What can you do with it? Take a look: Installation Updated October 27, 2021 Download the DART installer for your system. The current version is 2.0.16 (released October 27, 2021). Mac : https://s3.amazonaws.com/aptrust.public.download/DART/DART-2.0.16.dmg Windows : https://s3.amazonaws.com/aptrust.public.download/DART/DART+Setup+2.0.16.exe Linux : https://s3.amazonaws.com/aptrust.public.download/DART/DART_2.0.16_amd64.deb Double-click the installer after download and follow the prompts on screen. Set Up After installation, DART includes two BagIt profiles by default, one for APTrust and one for DPN. You can use one of these two profiles to create your first job. While jobs can include packaging, validation, and upload operations, you won't be able to upload anything until you've set up a Storage Service to receive an upload. With this, your first job will be limited to creating and validating a BagIt bag. Running Your First Job Follow these steps to create a valid local bag that conforms to the APTrust BagIt profile: Choose Jobs > New from the main menu. Drag some files or folders into the files window. To avoid a long-running job that will consume a lot of disk space, choose only a few megabytes of files. Click Next and set the following: Packaging Format : BagIt BagIt Profile : APTrust Package Name : test_bag Output Path : Don't edit this for now. DART will set it for you. Click Next and set the required metadata attributes, which are marked with a red asterisk. For an APTrust bag, these include the Access and Title tags in the aptrust-info.txt file, and the Source-Organization tag in the bag-info.txt file. Click Next to choose the upload targets. Since there will be no available upload targets after initial installtion, Click Next again. The Review and Run screen shows the details of the job you've just defined. Click Run Job to run the job. When the job is complete, you'll find your bag in the Output Path displayed on the Review and Run screen. Further Reading BagIt Profiles describes how to build and customize BagIt profiles, so DART can build bags exactly as you want them. Be sure to read the section on tag default values under Editing a Tag . That will save you from re-entering common tag values, such as Source-Organization, every time you create a new bag. The Dashboard displays information about currently running and recently completed jobs. If you've configured Remote Repository connections, the dashboard can show the status of recent and pending ingests. To get details about what DART is doing, check the Logs . The Settings section describes how to set up upload targets, remote repository connections, and more. Organizations that distribute DART to their members can Export Settings to help users quickly configure DART for their needs. After you've defined and tested a successful job, you can convert it to a Workflow to run other files through the same process.","title":"Getting Started"},{"location":"users/getting_started/#getting-started","text":"DART is the Digital Archivist's Resource Tool. Its primary purpose is to package digital materials and send them off to long-term preservation storage. DART's initial release focuses on packaging materials in BagIt format and uploading them to S3 buckets for ingest in APTrust. DART can be extended through plugins to produce other packaging formats and to communicated via additional network protocols. DART runs in both graphical and command-line modes on Windows, Mac, and Linux. What can you do with it? Take a look:","title":"Getting Started"},{"location":"users/getting_started/#installation","text":"Updated October 27, 2021 Download the DART installer for your system. The current version is 2.0.16 (released October 27, 2021). Mac : https://s3.amazonaws.com/aptrust.public.download/DART/DART-2.0.16.dmg Windows : https://s3.amazonaws.com/aptrust.public.download/DART/DART+Setup+2.0.16.exe Linux : https://s3.amazonaws.com/aptrust.public.download/DART/DART_2.0.16_amd64.deb Double-click the installer after download and follow the prompts on screen.","title":"Installation"},{"location":"users/getting_started/#set-up","text":"After installation, DART includes two BagIt profiles by default, one for APTrust and one for DPN. You can use one of these two profiles to create your first job. While jobs can include packaging, validation, and upload operations, you won't be able to upload anything until you've set up a Storage Service to receive an upload. With this, your first job will be limited to creating and validating a BagIt bag.","title":"Set Up"},{"location":"users/getting_started/#running-your-first-job","text":"Follow these steps to create a valid local bag that conforms to the APTrust BagIt profile: Choose Jobs > New from the main menu. Drag some files or folders into the files window. To avoid a long-running job that will consume a lot of disk space, choose only a few megabytes of files. Click Next and set the following: Packaging Format : BagIt BagIt Profile : APTrust Package Name : test_bag Output Path : Don't edit this for now. DART will set it for you. Click Next and set the required metadata attributes, which are marked with a red asterisk. For an APTrust bag, these include the Access and Title tags in the aptrust-info.txt file, and the Source-Organization tag in the bag-info.txt file. Click Next to choose the upload targets. Since there will be no available upload targets after initial installtion, Click Next again. The Review and Run screen shows the details of the job you've just defined. Click Run Job to run the job. When the job is complete, you'll find your bag in the Output Path displayed on the Review and Run screen.","title":"Running Your First Job"},{"location":"users/getting_started/#further-reading","text":"BagIt Profiles describes how to build and customize BagIt profiles, so DART can build bags exactly as you want them. Be sure to read the section on tag default values under Editing a Tag . That will save you from re-entering common tag values, such as Source-Organization, every time you create a new bag. The Dashboard displays information about currently running and recently completed jobs. If you've configured Remote Repository connections, the dashboard can show the status of recent and pending ingests. To get details about what DART is doing, check the Logs . The Settings section describes how to set up upload targets, remote repository connections, and more. Organizations that distribute DART to their members can Export Settings to help users quickly configure DART for their needs. After you've defined and tested a successful job, you can convert it to a Workflow to run other files through the same process.","title":"Further Reading"},{"location":"users/logs/","text":"Logs DART logs most of its activities as it works. If you're looking for detailed infomation about what DART is doing or has done, or if you want to see detailed error messages, check the logs. The easiest way to view the DART log is to click Help > Logs from the menu. This will display a live log window that shows updates as they are written. The About dialog will show the location of the DART log file.","title":"Logs"},{"location":"users/logs/#logs","text":"DART logs most of its activities as it works. If you're looking for detailed infomation about what DART is doing or has done, or if you want to see detailed error messages, check the logs. The easiest way to view the DART log is to click Help > Logs from the menu. This will display a live log window that shows updates as they are written. The About dialog will show the location of the DART log file.","title":"Logs"},{"location":"users/scripting/","text":"Scripting with DART The easiest way to script DART jobs is to: Create a workflow . Info If your workflow includes generating bags, you should also create a custom BagIt profile with default values so that your script will only need to create a few bag-specific tag values at runtime. See Customizing BagIt Profiles for more info. Create a simple data object that can be transformed into a JSON structure that matches DART's JobParams object. Pass the JSON to the DART command through STDIN. The examples below show how to do this in Ruby and Python. Note that both examples create the following JSON structure. { \"workflowName\" : \"DART Test Workflow\" , \"packageName\" : \"test.edu.my_files.tar\" , \"files\" : [ \"/Users/apd4n/aptrust/dart-docs/site\" , \"/Users/apd4n/tmp/logs\" ], \"tags\" : [{ \"tagFile\" : \"bag-info.txt\" , \"tagName\" : \"Bag-Group-Identifier\" , \"userValue\" : \"TestGroup_001\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Title\" , \"userValue\" : \"Workflow Test Files\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Description\" , \"userValue\" : \"Contains miscellaneous files for workflow testing.\" }] } Example: Ruby To script DART actions, you'll need one simple Ruby class that allows you to define and run a job. You can cut and paste this to get started, but note that you will likely have to change the value of @@dart_command to point to the DART executable on your local machine. require 'json' class Job # Be sure to set this appropriately for your system. # The command 'npm start' is for DART development use only. @@dart_command = 'npm start' attr_accessor :workflow_name , :package_name , :files , :tags def initialize ( workflow_name , package_name ) @workflow_name = workflow_name @package_name = package_name @files = [] @tags = [] end def to_json ( options = {}) { workflowName : workflow_name , packageName : package_name , files : files , tags : tags } . to_json end def add_file ( path ) @files << path end def add_tag ( tag_file , tag_name , value ) @tags << { tagFile : tag_file , tagName : tag_name , userValue : value } end def run json_string = self . to_json puts json_string puts \"Starting job\" opts = { external_encoding : \"UTF-8\" , err : [ :child , :out ] } IO . popen ( \" #{ @@dart_command } -- --stdin\" , \"r+\" , opts ) { | pipe | pipe . write json_string + \" \\n \" pipe . close_write puts pipe . read } return $? . exitstatus end end Assuming you saved the code above to a file called job.rb , you can create a script like the following to run a job based on a pre-defined DART workflow. require './job' # Create a new job using the \"DART Test Workflow.\" # This job will create a tarred bag called test.edu.my_files.tar # in your DART bagging directory. job = Job . new ( \"DART Test Workflow\" , \"test.edu.my_files.tar\" ) # Add two directories to the list of items that should go into # the bag's payload. Note that you can add a mix of files and # directories. job . add_file ( \"/Users/apd4n/aptrust/dart-docs/site\" ) job . add_file ( \"/Users/apd4n/tmp/logs\" ) # \"DART Test Workflow\" uses a BagIt profile with a number of # preset default values, which are fine for tags like Contact-Email, # which doesn't change from bag to bag. Here we set bag-specific # tag values. job . add_tag ( \"bag-info.txt\" , \"Bag-Group-Identifier\" , \"TestGroup_001\" ) job . add_tag ( \"aptrust-info.txt\" , \"Title\" , \"Workflow Test Files\" ) job . add_tag ( \"aptrust-info.txt\" , \"Description\" , \"Contains miscellaneous files for workflow testing.\" ) # Run the job and check the exit code. 0 indicates success. # Non-zero values indicate failure. exit_code = job . run () if exit_code == 0 puts \"Job completed\" else puts \"Job failed. Check the DART log for details.\" end Example: Python The following Python class will help you define and run DART jobs. You can cut and paste this to get started, but note that you will likely have to change the value of dart_command to point to the DART executable on your local machine. import json import sys from subprocess import Popen , PIPE class Job : # Be sure to set this appropriately for your system. # The command 'npm start' is for DART development use only. dart_command = 'npm start' def __init__ ( self , workflow_name , package_name ): self . workflow_name = workflow_name self . package_name = package_name self . files = [] self . tags = [] def add_file ( self , path ): self . files . append ( path ) def add_tag ( self , tag_file , tag_name , value ): self . tags . append ({ \"tagFile\" : tag_file , \"tagName\" : tag_name , \"userValue\" : value }) def to_json ( self ): _dict = { \"workflowName\" : self . workflow_name , \"packageName\" : self . package_name , \"files\" : self . files , \"tags\" : self . tags } return json . dumps ( _dict ) def run ( self ): json_string = self . to_json () print ( json_string ) print ( \"Starting job\" ) cmd = \" %s -- --stdin\" % Job . dart_command child = Popen ( cmd , shell = True , stdin = PIPE , stdout = PIPE , close_fds = True ) stdout_data , stderr_data = child . communicate ( json_string + \" \\n \" ) if stdout_data is not None : print ( stdout_data ) if stderr_data is not None : sys . stderr . write ( stderr_data ) return child . returncode Assuming you saved the code above to a file called job.py, you can create a script like the following to run a job based on a pre-defined DART workflow. from job import Job # Create a new job using the \"DART Test Workflow.\" # This job will create a tarred bag called test.edu.my_files.tar # in your DART bagging directory. job = Job ( \"DART Test Workflow\" , \"test.edu.my_files.tar\" ) # Add two directories to the list of items that should go into # the bag's payload. Note that you can add a mix of files and # directories. job . add_file ( \"/Users/apd4n/aptrust/dart-docs/site\" ) job . add_file ( \"/Users/apd4n/tmp/logs\" ) # \"DART Test Workflow\" uses a BagIt profile with a number of # preset default values, which are fine for tags like Contact-Email, # which doesn't change from bag to bag. Here we set bag-specific # tag values. job . add_tag ( \"bag-info.txt\" , \"Bag-Group-Identifier\" , \"TestGroup_001\" ) job . add_tag ( \"aptrust-info.txt\" , \"Title\" , \"Workflow Test Files\" ) job . add_tag ( \"aptrust-info.txt\" , \"Description\" , \"Contains miscellaneous files for workflow testing.\" ) # Run the job and check the exit code. 0 indicates success. # Non-zero values indicate failure. exit_code = job . run () if exit_code == 0 : print ( \"Job completed\" ) else : print ( \"Job failed. Check the DART log for details.\" )","title":"Scripting with DART"},{"location":"users/scripting/#scripting-with-dart","text":"The easiest way to script DART jobs is to: Create a workflow . Info If your workflow includes generating bags, you should also create a custom BagIt profile with default values so that your script will only need to create a few bag-specific tag values at runtime. See Customizing BagIt Profiles for more info. Create a simple data object that can be transformed into a JSON structure that matches DART's JobParams object. Pass the JSON to the DART command through STDIN. The examples below show how to do this in Ruby and Python. Note that both examples create the following JSON structure. { \"workflowName\" : \"DART Test Workflow\" , \"packageName\" : \"test.edu.my_files.tar\" , \"files\" : [ \"/Users/apd4n/aptrust/dart-docs/site\" , \"/Users/apd4n/tmp/logs\" ], \"tags\" : [{ \"tagFile\" : \"bag-info.txt\" , \"tagName\" : \"Bag-Group-Identifier\" , \"userValue\" : \"TestGroup_001\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Title\" , \"userValue\" : \"Workflow Test Files\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Description\" , \"userValue\" : \"Contains miscellaneous files for workflow testing.\" }] }","title":"Scripting with DART"},{"location":"users/scripting/#example-ruby","text":"To script DART actions, you'll need one simple Ruby class that allows you to define and run a job. You can cut and paste this to get started, but note that you will likely have to change the value of @@dart_command to point to the DART executable on your local machine. require 'json' class Job # Be sure to set this appropriately for your system. # The command 'npm start' is for DART development use only. @@dart_command = 'npm start' attr_accessor :workflow_name , :package_name , :files , :tags def initialize ( workflow_name , package_name ) @workflow_name = workflow_name @package_name = package_name @files = [] @tags = [] end def to_json ( options = {}) { workflowName : workflow_name , packageName : package_name , files : files , tags : tags } . to_json end def add_file ( path ) @files << path end def add_tag ( tag_file , tag_name , value ) @tags << { tagFile : tag_file , tagName : tag_name , userValue : value } end def run json_string = self . to_json puts json_string puts \"Starting job\" opts = { external_encoding : \"UTF-8\" , err : [ :child , :out ] } IO . popen ( \" #{ @@dart_command } -- --stdin\" , \"r+\" , opts ) { | pipe | pipe . write json_string + \" \\n \" pipe . close_write puts pipe . read } return $? . exitstatus end end Assuming you saved the code above to a file called job.rb , you can create a script like the following to run a job based on a pre-defined DART workflow. require './job' # Create a new job using the \"DART Test Workflow.\" # This job will create a tarred bag called test.edu.my_files.tar # in your DART bagging directory. job = Job . new ( \"DART Test Workflow\" , \"test.edu.my_files.tar\" ) # Add two directories to the list of items that should go into # the bag's payload. Note that you can add a mix of files and # directories. job . add_file ( \"/Users/apd4n/aptrust/dart-docs/site\" ) job . add_file ( \"/Users/apd4n/tmp/logs\" ) # \"DART Test Workflow\" uses a BagIt profile with a number of # preset default values, which are fine for tags like Contact-Email, # which doesn't change from bag to bag. Here we set bag-specific # tag values. job . add_tag ( \"bag-info.txt\" , \"Bag-Group-Identifier\" , \"TestGroup_001\" ) job . add_tag ( \"aptrust-info.txt\" , \"Title\" , \"Workflow Test Files\" ) job . add_tag ( \"aptrust-info.txt\" , \"Description\" , \"Contains miscellaneous files for workflow testing.\" ) # Run the job and check the exit code. 0 indicates success. # Non-zero values indicate failure. exit_code = job . run () if exit_code == 0 puts \"Job completed\" else puts \"Job failed. Check the DART log for details.\" end","title":"Example: Ruby"},{"location":"users/scripting/#example-python","text":"The following Python class will help you define and run DART jobs. You can cut and paste this to get started, but note that you will likely have to change the value of dart_command to point to the DART executable on your local machine. import json import sys from subprocess import Popen , PIPE class Job : # Be sure to set this appropriately for your system. # The command 'npm start' is for DART development use only. dart_command = 'npm start' def __init__ ( self , workflow_name , package_name ): self . workflow_name = workflow_name self . package_name = package_name self . files = [] self . tags = [] def add_file ( self , path ): self . files . append ( path ) def add_tag ( self , tag_file , tag_name , value ): self . tags . append ({ \"tagFile\" : tag_file , \"tagName\" : tag_name , \"userValue\" : value }) def to_json ( self ): _dict = { \"workflowName\" : self . workflow_name , \"packageName\" : self . package_name , \"files\" : self . files , \"tags\" : self . tags } return json . dumps ( _dict ) def run ( self ): json_string = self . to_json () print ( json_string ) print ( \"Starting job\" ) cmd = \" %s -- --stdin\" % Job . dart_command child = Popen ( cmd , shell = True , stdin = PIPE , stdout = PIPE , close_fds = True ) stdout_data , stderr_data = child . communicate ( json_string + \" \\n \" ) if stdout_data is not None : print ( stdout_data ) if stderr_data is not None : sys . stderr . write ( stderr_data ) return child . returncode Assuming you saved the code above to a file called job.py, you can create a script like the following to run a job based on a pre-defined DART workflow. from job import Job # Create a new job using the \"DART Test Workflow.\" # This job will create a tarred bag called test.edu.my_files.tar # in your DART bagging directory. job = Job ( \"DART Test Workflow\" , \"test.edu.my_files.tar\" ) # Add two directories to the list of items that should go into # the bag's payload. Note that you can add a mix of files and # directories. job . add_file ( \"/Users/apd4n/aptrust/dart-docs/site\" ) job . add_file ( \"/Users/apd4n/tmp/logs\" ) # \"DART Test Workflow\" uses a BagIt profile with a number of # preset default values, which are fine for tags like Contact-Email, # which doesn't change from bag to bag. Here we set bag-specific # tag values. job . add_tag ( \"bag-info.txt\" , \"Bag-Group-Identifier\" , \"TestGroup_001\" ) job . add_tag ( \"aptrust-info.txt\" , \"Title\" , \"Workflow Test Files\" ) job . add_tag ( \"aptrust-info.txt\" , \"Description\" , \"Contains miscellaneous files for workflow testing.\" ) # Run the job and check the exit code. 0 indicates success. # Non-zero values indicate failure. exit_code = job . run () if exit_code == 0 : print ( \"Job completed\" ) else : print ( \"Job failed. Check the DART log for details.\" )","title":"Example: Python"},{"location":"users/bagit/","text":"BagIt Profiles While the BagIt specification describes the general requirements for a valid bag, BagIt profiles describe the tags, manifests, and tag manifests required to make a valid bag for a specific organization or purpose. DART uses BagIt profiles to produce bags that adhere to a profile, and to validate that bags do adhere to the profile. DART BagIt Profiles While the general specification for BagIt Profiles can be found on GitHub , DART's BagIt profiles differ from the published specification in the following ways: DART profiles use camel case identifiers with no hyphens or periods in attribute names. For example, the Allow-Fetch.txt in GitHub BagIt profiles is called allowFetchTxt in DART profiles. This is in part to simplify JavaScript attributes so they can be reference in dot notation, and in part because the nedb object database used in early versions of DART did not support attribute names containing dots. DART profiles include an id attribute with a UUID. This is used internally, while externally the bagItProfileIdentifier URL is used externally. DART profiles include the following additional boolean attributes: allowMiscTopLevelFiles which indicates whether files other than manifests and tag manifests are allowed in the bag's root directory. allowMiscDirectories which indicates whether directories other than /data and its children are allowed in the bag. tarDirMustMatchName which indicates whether the name of the unserialized bag must match the name of the serialized bag, minus the serialization extension. (That is, whether my_bag.tar must untar to my_bag, and my_bag.zip must unzip to my_bag.) DART includes the attribute baseProfileId for internal use, to know whether a user-created profile was based on an existing profile. DART includes the isBuiltIn attribute to indicate that a profile was built in to the application (usually through a setup module or migration). These profiles cannot be deleted. DART does not specify tagFilesRequired or tagFilesAllowed . DART BagIt profiles specify all tag requirements in a single list called tags while the GitHub BagIt spec defines them as nested objects with arbitrary names. The single list of uniform objects in the DART model makes tag definitions easier to manipulate. While tag definitions in the GitHub BagIt Profile spec include only the attributes required and values , DART tag definitions include the following attributes: id - A unique identifier in UUID format that DART uses internally. This allows users to edit tag definitions in the DART UI without the system losing track of which tag is being edited. The UUID is immutable while all other attributes are not. tagFile - The name of the tag file that contains the tag. This is a path relative to the bag root. For example, bag-info.txt or custom-tags/image-credits.txt . tagName - The name of the tag. For example, Source-Organization . required - A boolean indicating whether the tag is required. values - An option list of legal values. If this list is present and a tag contains a value that's not in the list, the value and the bag are invalid. defaultValue - A default value assigned by the user to the tag. DART's BagIt Profile editor allows users to specify default values to tags that will be consistent across bags. For example, users can define a default value to Source-Organization so they don't have to assign it repeatedly every time they create a new bag. userValue - The value of a tag to be written into or read from a tag file. Users can specify a userValue that overrides the defaultValue when they create a bag. When reading a bag, DART assigns the actual value of a tag to what was read from the bag. isBuiltIn - A boolean value indicating whether a tag definition is built in (as opposed to user-created). DART's BagIt Profile editor allows users to add custom tags to a published profile, such as the APTrust profile, while preventing them from deleting built-in tag definitions. Deleting a built-in tag definition such as Source-Organization would lead to DART generating invalid bags. help - Help text to describe the significance of the tag to the user. If present, the DART UI will display this message for the user's edification and delight. Built-in Profiles DART includes the following built-in profiles. Note that the BagIt Profile editor allows you to clone and customize each of these, though customization is limited to adding tags and tag files, and setting default tag values. APTrust - The standard APTrust BagIt profile. BTR - The Beyond the Repository BagIt profile, which will be accepted by a number of distributed digital preservation repositories. Empty Profile - A BagIt profile that defines the basic tags of the BagIt specification but does not require any of them. You can clone use this profile to create valid generic bags. Custom Profiles DART enables users to create new BagIt profiles from scratch, and to clone and modify existing profiles. See also: Creating BagIt Profiles , Customizing BagIt Profiles","title":"BagIt Profiles"},{"location":"users/bagit/#bagit-profiles","text":"While the BagIt specification describes the general requirements for a valid bag, BagIt profiles describe the tags, manifests, and tag manifests required to make a valid bag for a specific organization or purpose. DART uses BagIt profiles to produce bags that adhere to a profile, and to validate that bags do adhere to the profile.","title":"BagIt Profiles"},{"location":"users/bagit/#dart-bagit-profiles","text":"While the general specification for BagIt Profiles can be found on GitHub , DART's BagIt profiles differ from the published specification in the following ways: DART profiles use camel case identifiers with no hyphens or periods in attribute names. For example, the Allow-Fetch.txt in GitHub BagIt profiles is called allowFetchTxt in DART profiles. This is in part to simplify JavaScript attributes so they can be reference in dot notation, and in part because the nedb object database used in early versions of DART did not support attribute names containing dots. DART profiles include an id attribute with a UUID. This is used internally, while externally the bagItProfileIdentifier URL is used externally. DART profiles include the following additional boolean attributes: allowMiscTopLevelFiles which indicates whether files other than manifests and tag manifests are allowed in the bag's root directory. allowMiscDirectories which indicates whether directories other than /data and its children are allowed in the bag. tarDirMustMatchName which indicates whether the name of the unserialized bag must match the name of the serialized bag, minus the serialization extension. (That is, whether my_bag.tar must untar to my_bag, and my_bag.zip must unzip to my_bag.) DART includes the attribute baseProfileId for internal use, to know whether a user-created profile was based on an existing profile. DART includes the isBuiltIn attribute to indicate that a profile was built in to the application (usually through a setup module or migration). These profiles cannot be deleted. DART does not specify tagFilesRequired or tagFilesAllowed . DART BagIt profiles specify all tag requirements in a single list called tags while the GitHub BagIt spec defines them as nested objects with arbitrary names. The single list of uniform objects in the DART model makes tag definitions easier to manipulate. While tag definitions in the GitHub BagIt Profile spec include only the attributes required and values , DART tag definitions include the following attributes: id - A unique identifier in UUID format that DART uses internally. This allows users to edit tag definitions in the DART UI without the system losing track of which tag is being edited. The UUID is immutable while all other attributes are not. tagFile - The name of the tag file that contains the tag. This is a path relative to the bag root. For example, bag-info.txt or custom-tags/image-credits.txt . tagName - The name of the tag. For example, Source-Organization . required - A boolean indicating whether the tag is required. values - An option list of legal values. If this list is present and a tag contains a value that's not in the list, the value and the bag are invalid. defaultValue - A default value assigned by the user to the tag. DART's BagIt Profile editor allows users to specify default values to tags that will be consistent across bags. For example, users can define a default value to Source-Organization so they don't have to assign it repeatedly every time they create a new bag. userValue - The value of a tag to be written into or read from a tag file. Users can specify a userValue that overrides the defaultValue when they create a bag. When reading a bag, DART assigns the actual value of a tag to what was read from the bag. isBuiltIn - A boolean value indicating whether a tag definition is built in (as opposed to user-created). DART's BagIt Profile editor allows users to add custom tags to a published profile, such as the APTrust profile, while preventing them from deleting built-in tag definitions. Deleting a built-in tag definition such as Source-Organization would lead to DART generating invalid bags. help - Help text to describe the significance of the tag to the user. If present, the DART UI will display this message for the user's edification and delight.","title":"DART BagIt Profiles"},{"location":"users/bagit/#built-in-profiles","text":"DART includes the following built-in profiles. Note that the BagIt Profile editor allows you to clone and customize each of these, though customization is limited to adding tags and tag files, and setting default tag values. APTrust - The standard APTrust BagIt profile. BTR - The Beyond the Repository BagIt profile, which will be accepted by a number of distributed digital preservation repositories. Empty Profile - A BagIt profile that defines the basic tags of the BagIt specification but does not require any of them. You can clone use this profile to create valid generic bags.","title":"Built-in Profiles"},{"location":"users/bagit/#custom-profiles","text":"DART enables users to create new BagIt profiles from scratch, and to clone and modify existing profiles. See also: Creating BagIt Profiles , Customizing BagIt Profiles","title":"Custom Profiles"},{"location":"users/bagit/creating/","text":"Creating Profiles To create a new BagIt Profile: Choose Settings > BagIt Profiles from the menu. Click the New button at the top of the profiles list. Select an option from the Base Profile list. Chose None if you want to create a new profile from scratch. Choose the name of an existing profile if you want to clone and modify an existing profile. Click the Create button. Note Cloning an existing BagIt profile can be useful if you're going to create bags on behalf of more than one organization or group. You can set different default values for tags such as Source-Organization or Contact-Email within each cloned profile, and then assign meaningful names such as APTrust Profile for Law Library and APTrust Profile for Engineering Library . Once you've created a new BagIt profile, you'll want to customize it using the built-in BagIt profile editor. See also: Customizing BagIt Profiles","title":"Creating Profiles"},{"location":"users/bagit/creating/#creating-profiles","text":"To create a new BagIt Profile: Choose Settings > BagIt Profiles from the menu. Click the New button at the top of the profiles list. Select an option from the Base Profile list. Chose None if you want to create a new profile from scratch. Choose the name of an existing profile if you want to clone and modify an existing profile. Click the Create button. Note Cloning an existing BagIt profile can be useful if you're going to create bags on behalf of more than one organization or group. You can set different default values for tags such as Source-Organization or Contact-Email within each cloned profile, and then assign meaningful names such as APTrust Profile for Law Library and APTrust Profile for Engineering Library . Once you've created a new BagIt profile, you'll want to customize it using the built-in BagIt profile editor. See also: Customizing BagIt Profiles","title":"Creating Profiles"},{"location":"users/bagit/customizing/","text":"Customizing Profiles To customize a BagIt profile, click the name of the profile in the profiles list, or click new and follow the steps to create a BagIt profile . About The About tab of the BagIt profile editor enables you to set a name and description for your profile. Info The Info tab includes fields to edit the BagIt-Profile-Info section of the profile. This includes the profile's URL identifier. General The General tab includes information about which BagIt versions your profile accepts, whether to allow fetch.txt files, and whether to allow miscellaneous top level files (arbitrary tag files directly under the root directory) and miscellaneous directories outside the payload (/data) directory. Manifests The Manifests tab specifies which manifests and tag manifests your profile requires. You can select multiple options from each list by holding down the Control key on Windows or the Command key on Mac while you click. Serialization The Serialization tab allows you to specify whether serialization is required, optional, or forbidden, as well as which serialization formats are supported. You can also specify here whether serialized bags must deserialize to a directory whose name matches the serialized file name. (For example, my_bag.tar must untar to my_bag and my_bag.zip must unzip to my_bag.) Tag Files The Tag Files tab includes a drop-down list for editing the profile's tag files, and for adding new tag files. Adding a New Tag File To add a new tag file: Click Add New Tag File on the drop-down list. Enter a name for the tag file. If the name includes slashes, the tag file will be created in a subdirectory beneath the bag's root directory. For example, custom-tags/photo-credits.txt would be placed in the bag's custom-tags directory. Click the Save button. Editing a Tag File To edit a tag file: Click the Tag Files tab. Select the name of the file you want to edit. Adding a Tag To add a tag to a tag file, click the New Tag button (visible in the screenshot above), then follow the steps in Editing a Tag below. Editing a Tag To edit a tag: Click the name of the tag you want to edit. Set the appropriate values in the dialog. Tag Name - The name of the tag. This is required. Required - A Yes/No value indicating whether the tag must have a value for the bag to be considered valid. Values - An optional list of allowed values for this tag. Default Value - An optional default value for this tag. Help - An optional help message. This message will be displayed to users who are filling out a bag's tag values in DART. Click the Save button. Deleting a Tag To delete a tag, click the red X to the right of the tag name in the tag list view. If the tag does not have a red X, it is a required tag from a published profile and cannot be deleted. When you delete the last tag of a tag file, DART deletes the tag file as well. Deleting a Tag File To delete a tag file, delete all of the tags in the file. See Deleting a Tag above.","title":"Customizing Profiles"},{"location":"users/bagit/customizing/#customizing-profiles","text":"To customize a BagIt profile, click the name of the profile in the profiles list, or click new and follow the steps to create a BagIt profile .","title":"Customizing Profiles"},{"location":"users/bagit/customizing/#about","text":"The About tab of the BagIt profile editor enables you to set a name and description for your profile.","title":"About"},{"location":"users/bagit/customizing/#info","text":"The Info tab includes fields to edit the BagIt-Profile-Info section of the profile. This includes the profile's URL identifier.","title":"Info"},{"location":"users/bagit/customizing/#general","text":"The General tab includes information about which BagIt versions your profile accepts, whether to allow fetch.txt files, and whether to allow miscellaneous top level files (arbitrary tag files directly under the root directory) and miscellaneous directories outside the payload (/data) directory.","title":"General"},{"location":"users/bagit/customizing/#manifests","text":"The Manifests tab specifies which manifests and tag manifests your profile requires. You can select multiple options from each list by holding down the Control key on Windows or the Command key on Mac while you click.","title":"Manifests"},{"location":"users/bagit/customizing/#serialization","text":"The Serialization tab allows you to specify whether serialization is required, optional, or forbidden, as well as which serialization formats are supported. You can also specify here whether serialized bags must deserialize to a directory whose name matches the serialized file name. (For example, my_bag.tar must untar to my_bag and my_bag.zip must unzip to my_bag.)","title":"Serialization"},{"location":"users/bagit/customizing/#tag-files","text":"The Tag Files tab includes a drop-down list for editing the profile's tag files, and for adding new tag files.","title":"Tag Files"},{"location":"users/bagit/customizing/#adding-a-new-tag-file","text":"To add a new tag file: Click Add New Tag File on the drop-down list. Enter a name for the tag file. If the name includes slashes, the tag file will be created in a subdirectory beneath the bag's root directory. For example, custom-tags/photo-credits.txt would be placed in the bag's custom-tags directory. Click the Save button.","title":"Adding a New Tag File"},{"location":"users/bagit/customizing/#editing-a-tag-file","text":"To edit a tag file: Click the Tag Files tab. Select the name of the file you want to edit.","title":"Editing a Tag File"},{"location":"users/bagit/customizing/#adding-a-tag","text":"To add a tag to a tag file, click the New Tag button (visible in the screenshot above), then follow the steps in Editing a Tag below.","title":"Adding a Tag"},{"location":"users/bagit/customizing/#editing-a-tag","text":"To edit a tag: Click the name of the tag you want to edit. Set the appropriate values in the dialog. Tag Name - The name of the tag. This is required. Required - A Yes/No value indicating whether the tag must have a value for the bag to be considered valid. Values - An optional list of allowed values for this tag. Default Value - An optional default value for this tag. Help - An optional help message. This message will be displayed to users who are filling out a bag's tag values in DART. Click the Save button.","title":"Editing a Tag"},{"location":"users/bagit/customizing/#deleting-a-tag","text":"To delete a tag, click the red X to the right of the tag name in the tag list view. If the tag does not have a red X, it is a required tag from a published profile and cannot be deleted. When you delete the last tag of a tag file, DART deletes the tag file as well.","title":"Deleting a Tag"},{"location":"users/bagit/customizing/#deleting-a-tag-file","text":"To delete a tag file, delete all of the tags in the file. See Deleting a Tag above.","title":"Deleting a Tag File"},{"location":"users/bagit/exporting/","text":"Exporting Profiles DART can export profiles to the BagIt Profiles 1.3 specification , though note that that specification does not support validation of tags outside the bagit.txt and bag-info.txt files. When exporting a DART profile to the 1.3 specification format, you will lose information describing how to validate tags outside of those files. To export: Choose Settings > BagIt Profiles from the main menu. In the profile list, click the profile you want to export. Click the Export Profile button at the bottom right of the profile detail page. Copy and paste the exported JSON into a file of your choice. Note the instructions at the top of the export page. If you plan to publish the exported profile, be sure to set the correct profile URL. When a BagIt validator tries to validate bags created with this profile, it will try to fetch the profile from the URL. If the URL is incorrect, validation will fail.","title":"Exporting Profiles"},{"location":"users/bagit/exporting/#exporting-profiles","text":"DART can export profiles to the BagIt Profiles 1.3 specification , though note that that specification does not support validation of tags outside the bagit.txt and bag-info.txt files. When exporting a DART profile to the 1.3 specification format, you will lose information describing how to validate tags outside of those files. To export: Choose Settings > BagIt Profiles from the main menu. In the profile list, click the profile you want to export. Click the Export Profile button at the bottom right of the profile detail page. Copy and paste the exported JSON into a file of your choice. Note the instructions at the top of the export page. If you plan to publish the exported profile, be sure to set the correct profile URL. When a BagIt validator tries to validate bags created with this profile, it will try to fetch the profile from the URL. If the URL is incorrect, validation will fail.","title":"Exporting Profiles"},{"location":"users/bagit/importing/","text":"Importing Profiles DART can import BagIt profiles that follow the BagIt Profiles 1.3 specification , ordered Library of Congress profiles, such as SANC State Profile , and unordered Library of Congress profiles like other-project-profile.json . To import a profile: Choose Settings > BagIt Profiles from the main menu. Click the Import Profile button in the top right corner. Option 1: Choose Import from URL . Type or paste the URL you want to import, then click Import . Option 2: Choose Import from JSON , paste the BagIt profile JSON you want to import, then click Import .","title":"Importing Profiles"},{"location":"users/bagit/importing/#importing-profiles","text":"DART can import BagIt profiles that follow the BagIt Profiles 1.3 specification , ordered Library of Congress profiles, such as SANC State Profile , and unordered Library of Congress profiles like other-project-profile.json . To import a profile: Choose Settings > BagIt Profiles from the main menu. Click the Import Profile button in the top right corner. Option 1: Choose Import from URL . Type or paste the URL you want to import, then click Import . Option 2: Choose Import from JSON , paste the BagIt profile JSON you want to import, then click Import .","title":"Importing Profiles"},{"location":"users/jobs/","text":"Jobs A job is a set of actions that may include one or more of the following: Packaging a number of files into a defined format, such as BagIt, tar, etc. Validating the package. Uploading the package to one or more remote locations. DART was initially designed for APTrust depositors to bag files according to the APTrust BagIt profile, validate the bags, and then send them to an S3 bucket for ingest into APTrust's preservation repository. DART plugin architecture will allow it to handle similary patterned jobs that use different packaging formats and network protocols such as zip, rar, or parchive formats sent via FTP or rsync. The process for creating and running jobs invlolves these steps: Adding files . Choosing a package format . Adding metadata . Choosing upload targets . Reviewing and running the job . Troubleshooting . For infomation about developing DART plugins, see the Plugins Developer Documentation . Jobs and Workflows Jobs can be converted to workflows. A workflow is essentially a job template that can be run in the DART UI or from the command line. A workflow enables you to run a number of jobs that all follow the same pattern (same packaging format, same default metadata values, and same upload targets).","title":"Jobs"},{"location":"users/jobs/#jobs","text":"A job is a set of actions that may include one or more of the following: Packaging a number of files into a defined format, such as BagIt, tar, etc. Validating the package. Uploading the package to one or more remote locations. DART was initially designed for APTrust depositors to bag files according to the APTrust BagIt profile, validate the bags, and then send them to an S3 bucket for ingest into APTrust's preservation repository. DART plugin architecture will allow it to handle similary patterned jobs that use different packaging formats and network protocols such as zip, rar, or parchive formats sent via FTP or rsync. The process for creating and running jobs invlolves these steps: Adding files . Choosing a package format . Adding metadata . Choosing upload targets . Reviewing and running the job . Troubleshooting . For infomation about developing DART plugins, see the Plugins Developer Documentation .","title":"Jobs"},{"location":"users/jobs/#jobs-and-workflows","text":"Jobs can be converted to workflows. A workflow is essentially a job template that can be run in the DART UI or from the command line. A workflow enables you to run a number of jobs that all follow the same pattern (same packaging format, same default metadata values, and same upload targets).","title":"Jobs and Workflows"},{"location":"users/jobs/delete/","text":"Deleting Jobs To delete a job: Choose Jobs > List from the menu. Click on the job you want to delete. Click the red Delete button in the bottom left corner of the Job files page.","title":"Deleting Jobs"},{"location":"users/jobs/delete/#deleting-jobs","text":"To delete a job: Choose Jobs > List from the menu. Click on the job you want to delete. Click the red Delete button in the bottom left corner of the Job files page.","title":"Deleting Jobs"},{"location":"users/jobs/files/","text":"Adding Files After you create a new job or click on a job in the Jobs list, you'll see the files page. You can add files to a job by dragging them into the drop zone, which is outlined in blue. The files window shows the total number of files and directories you've added, and total size of all the files. After adding files and directories, click the Next button to move on to the Packaging step. Note that you can also delete the job from this screen. Removing Files To remove a file from the job, click the red X in the row of the file you want to delete.","title":"Adding Files"},{"location":"users/jobs/files/#adding-files","text":"After you create a new job or click on a job in the Jobs list, you'll see the files page. You can add files to a job by dragging them into the drop zone, which is outlined in blue. The files window shows the total number of files and directories you've added, and total size of all the files. After adding files and directories, click the Next button to move on to the Packaging step. Note that you can also delete the job from this screen.","title":"Adding Files"},{"location":"users/jobs/files/#removing-files","text":"To remove a file from the job, click the red X in the row of the file you want to delete.","title":"Removing Files"},{"location":"users/jobs/list/","text":"Listing Jobs To list all jobs, Choose Jobs > List from the menu. The list shows the job name and information about when it was last packaged, validated, and/or uploaded. Click on any job in the list to view it. On the following screen, you'll be able to edit or delete the job.","title":"Listing Jobs"},{"location":"users/jobs/list/#listing-jobs","text":"To list all jobs, Choose Jobs > List from the menu. The list shows the job name and information about when it was last packaged, validated, and/or uploaded. Click on any job in the list to view it. On the following screen, you'll be able to edit or delete the job.","title":"Listing Jobs"},{"location":"users/jobs/metadata/","text":"Metadata You'll see the job metadata screen if you chose the BagIt packaging format on the previous screen. This screen allows you to enter values for tags that will go into BagIt tag files. By default, this screen only shows tags whose values are not already filled in by default values. Default values come from two places: Default tag values you've entered into your local BagIt profiles. See the Editing a Tag section for information about setting default values. Auto-filled values calculated by DART when it creates the bag, such as the Payload-Oxum, Bagging-Software, and Bagging-Date. If you want to edit tags with pre-set default values, click the Show All Tags button in the top right corner of the page. Adding Custom Tags You can add a custom tag to a bag by clicking the Add New Tag button at the top of the page. Note that adding extra tags to a bag generally will not cause the bag to be invalid. The only exception to this rule is adding tags that the BagIt specification says may not be repeated, such as the Payload-Oxum tag.","title":"Metadata"},{"location":"users/jobs/metadata/#metadata","text":"You'll see the job metadata screen if you chose the BagIt packaging format on the previous screen. This screen allows you to enter values for tags that will go into BagIt tag files. By default, this screen only shows tags whose values are not already filled in by default values. Default values come from two places: Default tag values you've entered into your local BagIt profiles. See the Editing a Tag section for information about setting default values. Auto-filled values calculated by DART when it creates the bag, such as the Payload-Oxum, Bagging-Software, and Bagging-Date. If you want to edit tags with pre-set default values, click the Show All Tags button in the top right corner of the page.","title":"Metadata"},{"location":"users/jobs/metadata/#adding-custom-tags","text":"You can add a custom tag to a bag by clicking the Add New Tag button at the top of the page. Note that adding extra tags to a bag generally will not cause the bag to be invalid. The only exception to this rule is adding tags that the BagIt specification says may not be repeated, such as the Payload-Oxum tag.","title":"Adding Custom Tags"},{"location":"users/jobs/packaging/","text":"Packaging The packaging screen incudes the following options: Package Format - Choose how you want your files to be packaged. The following options are available: BagIt - Choose this if you want to bag your files in BagIt format. Tar - Choose this if you want to pack your files into a single tar file. BagIt Profile - This option appears when you choose BagIt as the package format. Note that you can have structurally identical BagIt profiles with different sets of default values. See Creating BagIt Profiles for more information. Serialization - Choose whether you want the bag to be serialized to tar format. We expect to support additional options in the future, including zip, gzip and others. Package Name - Type the name of the package you want to create. DART will create a package with this file name. Note that some repositories, including APTrust * , have required naming conventions, and most discourage the use of non-printable characters in package names. Output Path - This is where DART will put the local copy of the package that it builds. Notice that this field is filled in automatically as you type the package name. Unless you have good reason, you should not manually edit this field. * APTrust bag names should begin with your organization identifier, followed by a dot and a unique name. For example, virginia.edu.photos-2019-07-21.","title":"Packaging"},{"location":"users/jobs/packaging/#packaging","text":"The packaging screen incudes the following options: Package Format - Choose how you want your files to be packaged. The following options are available: BagIt - Choose this if you want to bag your files in BagIt format. Tar - Choose this if you want to pack your files into a single tar file. BagIt Profile - This option appears when you choose BagIt as the package format. Note that you can have structurally identical BagIt profiles with different sets of default values. See Creating BagIt Profiles for more information. Serialization - Choose whether you want the bag to be serialized to tar format. We expect to support additional options in the future, including zip, gzip and others. Package Name - Type the name of the package you want to create. DART will create a package with this file name. Note that some repositories, including APTrust * , have required naming conventions, and most discourage the use of non-printable characters in package names. Output Path - This is where DART will put the local copy of the package that it builds. Notice that this field is filled in automatically as you type the package name. Unless you have good reason, you should not manually edit this field. * APTrust bag names should begin with your organization identifier, followed by a dot and a unique name. For example, virginia.edu.photos-2019-07-21.","title":"Packaging"},{"location":"users/jobs/run/","text":"Run Your Job After defining the files your job will work with and the optional packaging, metadata, and upload steps, the Run page displays a summary of the work that DART will perform. Review the details to ensure that everything looks right, then click the Run button to run the job. After clicking run, your job will begin. DART runs each job in a separate process, which provides a number of benefits: You can run many jobs at once. If any job encounters errors, it will not affect the other running jobs. You can continue to use DART while background jobs run. Nothing you do in DART, except quitting the application, will affect the running jobs. After clicking Run , you'll see the progress of each operation on screen. Striped blue bars indicate a work in progress. A green bar represents a completed operation, while a red bar indicates a failure. If a job includes multiple uploads, you'll see one progress bar for each upload. Large jobs may take a long time to complete. As long as jobs are running, you can continue to work in DART without affecting their progress. You'll see a badge like this in the upper right corner of the menu showing the number of running background jobs: Progress Bar Accuracy The progress bars for packaging and validation are accurate to within a few seconds of actual operation time. The progress bar for uploads usually displays more progress than has actually been completed. This is because DART can know exactly how many bytes of an upload it has prepared to send, but not how many the upload target has received. The upload progress bar displays the number of bytes prepared, and will appear to stall at around 98% completion on large uploads, as it awaits transferral of the final chunk of data. In a 100 MB upload, that final chunk may include only 1 MB of data, taking a few seconds to transfer. In a 5 TB upload to S3, the final chunk can be 535 MB and can take many minutes to complete. Creating a Workflow from a Job If you've created a successful job that you want to be the pattern for future jobs, click the Create Workflow button to create a repeatable workflow that can run new files through the same set of steps that this job just completed.","title":"Run Your Job"},{"location":"users/jobs/run/#run-your-job","text":"After defining the files your job will work with and the optional packaging, metadata, and upload steps, the Run page displays a summary of the work that DART will perform. Review the details to ensure that everything looks right, then click the Run button to run the job. After clicking run, your job will begin. DART runs each job in a separate process, which provides a number of benefits: You can run many jobs at once. If any job encounters errors, it will not affect the other running jobs. You can continue to use DART while background jobs run. Nothing you do in DART, except quitting the application, will affect the running jobs. After clicking Run , you'll see the progress of each operation on screen. Striped blue bars indicate a work in progress. A green bar represents a completed operation, while a red bar indicates a failure. If a job includes multiple uploads, you'll see one progress bar for each upload. Large jobs may take a long time to complete. As long as jobs are running, you can continue to work in DART without affecting their progress. You'll see a badge like this in the upper right corner of the menu showing the number of running background jobs:","title":"Run Your Job"},{"location":"users/jobs/run/#progress-bar-accuracy","text":"The progress bars for packaging and validation are accurate to within a few seconds of actual operation time. The progress bar for uploads usually displays more progress than has actually been completed. This is because DART can know exactly how many bytes of an upload it has prepared to send, but not how many the upload target has received. The upload progress bar displays the number of bytes prepared, and will appear to stall at around 98% completion on large uploads, as it awaits transferral of the final chunk of data. In a 100 MB upload, that final chunk may include only 1 MB of data, taking a few seconds to transfer. In a 5 TB upload to S3, the final chunk can be 535 MB and can take many minutes to complete.","title":"Progress Bar Accuracy"},{"location":"users/jobs/run/#creating-a-workflow-from-a-job","text":"If you've created a successful job that you want to be the pattern for future jobs, click the Create Workflow button to create a repeatable workflow that can run new files through the same set of steps that this job just completed.","title":"Creating a Workflow from a Job"},{"location":"users/jobs/troubleshooting/","text":"Troubleshooting Jobs Note: APTrust will be adding to this section as issues arise. Uploading SFTP Bad path: parent not exist This can occur if the bucket name in your Storage Service record does not match the name of the upload folder on the sftp server. For example, if uploads should go into a folder on the server called uploads , but your StorageService has the bucket name upload , you'll see this error. See Storage Services for details on how to change Storage Service settings. Upload failed due to unknown error. This can occur when your local machine's SSH hosts file does not have the public key of the sftp server you are connecting to. You can fix this by manually connecting to the server with local : ~ yoozer $ ssh server . example . com The authenticity of host 'server.example.com (::1)' can 't be established. ED25519 key fingerprint is SHA256:3WxPp9MParT+tsW/LpcNWR1m4c126aWIR98LVyEgfcw. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added ' localhost ' (ED25519) to the list of known hosts. yoozer@server.example.com' s password : You can hit Control-C after the last prompt, since your ssh hosts file now has the key.","title":"Troubleshooting Jobs"},{"location":"users/jobs/troubleshooting/#troubleshooting-jobs","text":"Note: APTrust will be adding to this section as issues arise.","title":"Troubleshooting Jobs"},{"location":"users/jobs/troubleshooting/#uploading","text":"","title":"Uploading"},{"location":"users/jobs/troubleshooting/#sftp","text":"","title":"SFTP"},{"location":"users/jobs/troubleshooting/#bad-path-parent-not-exist","text":"This can occur if the bucket name in your Storage Service record does not match the name of the upload folder on the sftp server. For example, if uploads should go into a folder on the server called uploads , but your StorageService has the bucket name upload , you'll see this error. See Storage Services for details on how to change Storage Service settings.","title":"Bad path: parent not exist"},{"location":"users/jobs/troubleshooting/#upload-failed-due-to-unknown-error","text":"This can occur when your local machine's SSH hosts file does not have the public key of the sftp server you are connecting to. You can fix this by manually connecting to the server with local : ~ yoozer $ ssh server . example . com The authenticity of host 'server.example.com (::1)' can 't be established. ED25519 key fingerprint is SHA256:3WxPp9MParT+tsW/LpcNWR1m4c126aWIR98LVyEgfcw. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added ' localhost ' (ED25519) to the list of known hosts. yoozer@server.example.com' s password : You can hit Control-C after the last prompt, since your ssh hosts file now has the key.","title":"Upload failed due to unknown error."},{"location":"users/jobs/upload/","text":"Uploads You can specify one or more upload targets on the uploads page. Simply check the box beside each target you want to upload to. The list of available targets comes from the Storage Service settings in your local DART installation. You can add as many storage services as you like. To appear in the list of upload targets, a storage service must include a protocol, URL, login, and password, and it must have the Allows Upload set to Yes . If some of your storage services do not appear in the list, check to see they meet all of these criteria.","title":"Uploads"},{"location":"users/jobs/upload/#uploads","text":"You can specify one or more upload targets on the uploads page. Simply check the box beside each target you want to upload to. The list of available targets comes from the Storage Service settings in your local DART installation. You can add as many storage services as you like. To appear in the list of upload targets, a storage service must include a protocol, URL, login, and password, and it must have the Allows Upload set to Yes . If some of your storage services do not appear in the list, check to see they meet all of these criteria.","title":"Uploads"},{"location":"users/jobs/validation/","text":"Validation Jobs If you simply want to validate an existing bag, follow these steps: Choose Jobs > Validate a Bag from the menu. Choose the BagIt profile against which you want to validate. If you want to validate against the BagIt specification instead of a specific profile, choose the \"Empty Profile.\" Choose whether you want to validate a tarred or untarred bag. As of version 2.0.4 (March, 2020), DART supports only tarred and unserialized bags (i.e. a bag that is a folder). We plan to support additional formats in the future. Click Browse to choose the folder or tar file you want to validate. Click Validate . The progress bar will show the progress of the job, and DART will display specific error messages below the progress bar when the job is complete. Note that a bag that is valid according to one profile may be invalid according to others. If a bag is valid according to the Empty Profile, it conforms to the general IETF BagIt specification . A Note on Mac OS Dot-Underscore Files DART may report a number of errors for tarred bags created on Mac OS, stating that .DS_Store files or files beginning with \"._\" were found in the payload but not in the manifests. Typically, there will be one dot-underscore file for each payload file, like so: data/article.pdf -> data/._article.pdf data/image.jpg -> data/._image.jpg These files contain metadata used by the Mac OS filesystem. When you tar a bag with the typical command, tar -cf mybag.tar mybag , Mac includes the dot-underscore files by default, but your bagging software may not include those hidden files in the manifests, so the DART validator considers the bags invalid, with messages like \"File data/._image.jpg found in data directory is not present in manifest-sha256.txt.\" When tarring your own bags outside of DART, you get around this problem with the command COPYFILE_DISABLE=1 tar -cf mybag.tar mybag . That tells Mac's tar program to exclude dot-underscore files from the tarball.","title":"Validation Jobs"},{"location":"users/jobs/validation/#validation-jobs","text":"If you simply want to validate an existing bag, follow these steps: Choose Jobs > Validate a Bag from the menu. Choose the BagIt profile against which you want to validate. If you want to validate against the BagIt specification instead of a specific profile, choose the \"Empty Profile.\" Choose whether you want to validate a tarred or untarred bag. As of version 2.0.4 (March, 2020), DART supports only tarred and unserialized bags (i.e. a bag that is a folder). We plan to support additional formats in the future. Click Browse to choose the folder or tar file you want to validate. Click Validate . The progress bar will show the progress of the job, and DART will display specific error messages below the progress bar when the job is complete. Note that a bag that is valid according to one profile may be invalid according to others. If a bag is valid according to the Empty Profile, it conforms to the general IETF BagIt specification .","title":"Validation Jobs"},{"location":"users/jobs/validation/#a-note-on-mac-os-dot-underscore-files","text":"DART may report a number of errors for tarred bags created on Mac OS, stating that .DS_Store files or files beginning with \"._\" were found in the payload but not in the manifests. Typically, there will be one dot-underscore file for each payload file, like so: data/article.pdf -> data/._article.pdf data/image.jpg -> data/._image.jpg These files contain metadata used by the Mac OS filesystem. When you tar a bag with the typical command, tar -cf mybag.tar mybag , Mac includes the dot-underscore files by default, but your bagging software may not include those hidden files in the manifests, so the DART validator considers the bags invalid, with messages like \"File data/._image.jpg found in data directory is not present in manifest-sha256.txt.\" When tarring your own bags outside of DART, you get around this problem with the command COPYFILE_DISABLE=1 tar -cf mybag.tar mybag . That tells Mac's tar program to exclude dot-underscore files from the tarball.","title":"A Note on Mac OS Dot-Underscore Files"},{"location":"users/settings/","text":"Settings DART includes the following settings: App Settings include information such as your organization name (used when creating bags) and your default output directory (to which bags are written). Internal Settings are read-only settings that you may occasionally need to examine when tracing problems. Remote Repositories describe how to connect to remote preservation repositories that ingest the materials you upload. Storage Services describe how to connect to drop-off and pick up points, such as S3 buckets or FTP services. These are usually temporary storage points used to drop off materials for ingest or to pick up items restored by a preservation repository. You can also share settings with other users by importing and exporting them as in the video below.","title":"Settings"},{"location":"users/settings/#settings","text":"DART includes the following settings: App Settings include information such as your organization name (used when creating bags) and your default output directory (to which bags are written). Internal Settings are read-only settings that you may occasionally need to examine when tracing problems. Remote Repositories describe how to connect to remote preservation repositories that ingest the materials you upload. Storage Services describe how to connect to drop-off and pick up points, such as S3 buckets or FTP services. These are usually temporary storage points used to drop off materials for ingest or to pick up items restored by a preservation repository. You can also share settings with other users by importing and exporting them as in the video below.","title":"Settings"},{"location":"users/settings/app_settings/","text":"App Settings App Settings contain DART's application-wide settings. These may be used when creating bags and other packages. To view the list of all settings, select Settings > App Settings from the main menu. Editing App Settings Click on any setting in the list to edit it. Note that some essential settings, such as Bagging Directory and Institution Domain cannot be renamed or deleted, though their values can be changed.","title":"App Settings"},{"location":"users/settings/app_settings/#app-settings","text":"App Settings contain DART's application-wide settings. These may be used when creating bags and other packages. To view the list of all settings, select Settings > App Settings from the main menu.","title":"App Settings"},{"location":"users/settings/app_settings/#editing-app-settings","text":"Click on any setting in the list to edit it. Note that some essential settings, such as Bagging Directory and Institution Domain cannot be renamed or deleted, though their values can be changed.","title":"Editing App Settings"},{"location":"users/settings/export/","text":"Exporting Settings If you're setting up DART for a number of users, you may want to configure your local DART installation, then export your settings for others to use. For example, you can set up a BagIt Profile, application settings, remote repositories and storage services for your institution, then publish them to a URL for others to install via DART's import feature. DART Does Not Export Credentials When you export StorageService and RemoteRepository records, DART will not export login and password fields, unless they begin with \"env:\", which indicates the logins/passwords are to be loaded from the environment. You may still export these records, but you will then have to pass login and password information through another channel, such as phone, email, or PrivNote . Also note that if you export StorageService and RemoteRepository records and then import them back into your own DART installation, you may overwrite the records' existing login and password info with blank data. To export settings from DART: Choose Settings > Export Settings from the main menu. Check the boxes next to the settings you want to export. Click Export . After exporting your settings, click Copy to Clipboard to copy them. You can then post the settings to a public URL, or email them to other users for import. Export Questions If you are exporting settings for a number of other users, you may want to add questions to your export, to help walk users through the setup process. Because settings are meant to be published and shared, DART does not export login names and passwords. You should pass those by phone, PrivNote, or some other secure channel. You can define questions that will prompt the user for that information and DART will copy the user's response to the setting and field you define. For example, you can ask a user to enter their SFTP password and tell DART to copy into the connection settings for your SFTP server. DART can also copy values into the default value fields of BagIt profile tags. For example, if your BagIt profile includes an \"Internal-Department\" tag, user can enter their department name in response to a question and have the response become the default value for all bags they produce. To define setup questions: Check the box next to Add questions to help users import these settings. Click the Add Question button on the bottom of the Settings Export screen. Type in your question. For example, \"What is your password for the sftp server?\" Tell DART to copy the user's response into the password field of the Storage Service called \"SFTP demo server\". DART will present these questions to users when they import your settings. For more info, see importing settings .","title":"Exporting Settings"},{"location":"users/settings/export/#exporting-settings","text":"If you're setting up DART for a number of users, you may want to configure your local DART installation, then export your settings for others to use. For example, you can set up a BagIt Profile, application settings, remote repositories and storage services for your institution, then publish them to a URL for others to install via DART's import feature. DART Does Not Export Credentials When you export StorageService and RemoteRepository records, DART will not export login and password fields, unless they begin with \"env:\", which indicates the logins/passwords are to be loaded from the environment. You may still export these records, but you will then have to pass login and password information through another channel, such as phone, email, or PrivNote . Also note that if you export StorageService and RemoteRepository records and then import them back into your own DART installation, you may overwrite the records' existing login and password info with blank data. To export settings from DART: Choose Settings > Export Settings from the main menu. Check the boxes next to the settings you want to export. Click Export . After exporting your settings, click Copy to Clipboard to copy them. You can then post the settings to a public URL, or email them to other users for import.","title":"Exporting Settings"},{"location":"users/settings/export/#export-questions","text":"If you are exporting settings for a number of other users, you may want to add questions to your export, to help walk users through the setup process. Because settings are meant to be published and shared, DART does not export login names and passwords. You should pass those by phone, PrivNote, or some other secure channel. You can define questions that will prompt the user for that information and DART will copy the user's response to the setting and field you define. For example, you can ask a user to enter their SFTP password and tell DART to copy into the connection settings for your SFTP server. DART can also copy values into the default value fields of BagIt profile tags. For example, if your BagIt profile includes an \"Internal-Department\" tag, user can enter their department name in response to a question and have the response become the default value for all bags they produce. To define setup questions: Check the box next to Add questions to help users import these settings. Click the Add Question button on the bottom of the Settings Export screen. Type in your question. For example, \"What is your password for the sftp server?\" Tell DART to copy the user's response into the password field of the Storage Service called \"SFTP demo server\". DART will present these questions to users when they import your settings. For more info, see importing settings .","title":"Export Questions"},{"location":"users/settings/import/","text":"Importing Settings You can import settings into DART from a URL or by copying and pasting data. DART's data import feature helps organizations configure and distribute basic settings for their users. DART Does Not Export Credentials By default, DART will not export login and password fields. When sharing settings, DART's import feature will pull in most of the data you need. However, you will have to get login and password information through another channel, such as phone, email, or PrivNote . To import settings, follow these steps: Choose Settings > Import Settings from the menu. Choose whether to import settings from a URL or from copy-and-paste text. If you choose URL, enter the URL in the text box. If you choose \"Import from JSON\", paste the JSON into the text area. Click Import . When the import is complete, DART shows a list of the imported settings, and an optional set of questions. You should answer the questions to complete the setup. DART will copy your answers to the correct settings.","title":"Importing Settings"},{"location":"users/settings/import/#importing-settings","text":"You can import settings into DART from a URL or by copying and pasting data. DART's data import feature helps organizations configure and distribute basic settings for their users. DART Does Not Export Credentials By default, DART will not export login and password fields. When sharing settings, DART's import feature will pull in most of the data you need. However, you will have to get login and password information through another channel, such as phone, email, or PrivNote . To import settings, follow these steps: Choose Settings > Import Settings from the menu. Choose whether to import settings from a URL or from copy-and-paste text. If you choose URL, enter the URL in the text box. If you choose \"Import from JSON\", paste the JSON into the text area. Click Import . When the import is complete, DART shows a list of the imported settings, and an optional set of questions. You should answer the questions to complete the setup. DART will copy your answers to the correct settings.","title":"Importing Settings"},{"location":"users/settings/internal_settings/","text":"Internal Settings Internal Settings contain configuration information that users cannot edit. These settings are created by plugins, settings imports, and software updates. Though you cannot change them, knowing their values may be helpful to DART users and developers.","title":"Internal Settings"},{"location":"users/settings/internal_settings/#internal-settings","text":"Internal Settings contain configuration information that users cannot edit. These settings are created by plugins, settings imports, and software updates. Though you cannot change them, knowing their values may be helpful to DART users and developers.","title":"Internal Settings"},{"location":"users/settings/plugins/","text":"Plugins DART plugins provide features such as the following: Format Reader - Allows DART to read files packaged in a certain format, such as tar. Format Writer - Allows DART to write files into specific formats, such as tar. Network Client - Allows DART to communicate via certain network protocols, such as S3, so that it can upload and/or download files. Repository - Allows DART to communicate with remote repositories to retrieve information such as a list of ingested objects. To see the list of plugins installed on your system, choose Settings > Plugins from the menu. You should see a list like this: See also: Plugins documentation for developers","title":"Plugins"},{"location":"users/settings/plugins/#plugins","text":"DART plugins provide features such as the following: Format Reader - Allows DART to read files packaged in a certain format, such as tar. Format Writer - Allows DART to write files into specific formats, such as tar. Network Client - Allows DART to communicate via certain network protocols, such as S3, so that it can upload and/or download files. Repository - Allows DART to communicate with remote repositories to retrieve information such as a list of ingested objects. To see the list of plugins installed on your system, choose Settings > Plugins from the menu. You should see a list like this: See also: Plugins documentation for developers","title":"Plugins"},{"location":"users/settings/remote_repositories/","text":"Remote Repositories Remote repositories are services to which you upload data for preservation. DART can query remote repositories to show the status of items you've uploaded for ingest, provided the following three conditions are met: The remote repository has a REST API. Most do, including APTrust, Fedora, DSpace, and many others. DART has a plugin that knows how to talk to the repository. (On initial release, the only plugin is for APTrust, but more may be coming.) You have a Remote Repository setting that points to the correct URL and contains valid login credentials. Listing Remote Repositories To view the list of Remote Repository settings, choose Settings > Remote Repositories from the menu. Editing Remote Repositories Click on any repository in the list to edit it, or click the New button to create a new one. Description of Settings Name The name of the remote repository. This is required. It can be anything you want, and chaning it will not affect the behavior or availability of the repository. URL The base URL of the repository's REST API. This may or may not include path information. For example, https://example.com has no path information, while https://example.com/api/v2/ does include path info. Check with your repository to get the correct URL. Plugin Choose the plugin that can connect to your repository. If you don't see the plugin in the list, it has not been installed. Note that at the time of DART's initial release, the only available plugin is APTrustClient . User ID Enter the user ID you use to connect to the repository's REST API. If the API only uses a token and no user ID, leave this blank. API Token Enter the API token used to connect to this repository. You'll have to get a token from the repository itself. Login Extra This field is generally left blank. If your repository uses it, they should provide instructions on how to fill this in.","title":"Remote Repositories"},{"location":"users/settings/remote_repositories/#remote-repositories","text":"Remote repositories are services to which you upload data for preservation. DART can query remote repositories to show the status of items you've uploaded for ingest, provided the following three conditions are met: The remote repository has a REST API. Most do, including APTrust, Fedora, DSpace, and many others. DART has a plugin that knows how to talk to the repository. (On initial release, the only plugin is for APTrust, but more may be coming.) You have a Remote Repository setting that points to the correct URL and contains valid login credentials.","title":"Remote Repositories"},{"location":"users/settings/remote_repositories/#listing-remote-repositories","text":"To view the list of Remote Repository settings, choose Settings > Remote Repositories from the menu.","title":"Listing Remote Repositories"},{"location":"users/settings/remote_repositories/#editing-remote-repositories","text":"Click on any repository in the list to edit it, or click the New button to create a new one.","title":"Editing Remote Repositories"},{"location":"users/settings/remote_repositories/#description-of-settings","text":"","title":"Description of Settings"},{"location":"users/settings/remote_repositories/#name","text":"The name of the remote repository. This is required. It can be anything you want, and chaning it will not affect the behavior or availability of the repository.","title":"Name"},{"location":"users/settings/remote_repositories/#url","text":"The base URL of the repository's REST API. This may or may not include path information. For example, https://example.com has no path information, while https://example.com/api/v2/ does include path info. Check with your repository to get the correct URL.","title":"URL"},{"location":"users/settings/remote_repositories/#plugin","text":"Choose the plugin that can connect to your repository. If you don't see the plugin in the list, it has not been installed. Note that at the time of DART's initial release, the only available plugin is APTrustClient .","title":"Plugin"},{"location":"users/settings/remote_repositories/#user-id","text":"Enter the user ID you use to connect to the repository's REST API. If the API only uses a token and no user ID, leave this blank.","title":"User ID"},{"location":"users/settings/remote_repositories/#api-token","text":"Enter the API token used to connect to this repository. You'll have to get a token from the repository itself.","title":"API Token"},{"location":"users/settings/remote_repositories/#login-extra","text":"This field is generally left blank. If your repository uses it, they should provide instructions on how to fill this in.","title":"Login Extra"},{"location":"users/settings/storage_services/","text":"Storage Services Storage services are not repositories! They are pickup and drop-off points for materials going into or coming out of repositories. Some repositories ask depositors to upload materials into an S3 bucket or an SFTP folder for ingest, and restore materials to a similar bucket or folder for depositors to retrieve. Storage services allow DART to connect to these pickup and drop-off points. Note, however, that you're free to send data to and from these storage services even if they're not ultimately bound for a preservation repository. Listing Storage Services To list all storage services, choose Settings > Storage Services from the menu. Editing Storage Services Click any storage servicei in the list to edit it. Description of Settings Name The name of the service. Choose a name that's meaningful to you and that differentiates this service from others. You can change the name at any time without affecting the bevavior or availability of the service. Description A description of this service. Protocol Choose the network protocol used to communicate with this service. Note: At launch, DART supports only the S3 protocol. Host Enter the name or IP address of the service host. Do not include protocol prefixes like https:// or ftp:// . For example, the host name for Amazon's S3 service is s3.amazonaws.com . A locally hosted service may be s3.example.com or ftp.example.com . You can also enter an IP address here such as 127.0.0.1 . Port The port to connect to. In most cases, you'll want to leave this at 0 (zero). Set this only if the service is running on a non-standard port number. Bucket The name of the bucket you'll be uploading into or downloading from on the remote host. For the S3 protocol, this will be a bucket name like aptrust.dart.test . (If you are an APTrust member, your bucket name will be aptrust.receiving.test. for the demo system and aptrust.receiving. for the production system. For example, receiving buckets for the University of Virginia would be aptrust.test.receiving.virginia.edu for demo and aptrust.receiving.virginia.edu for production.) For protocols like FTP and rsync, this Bucket setting should be a directory name like uploads/ingest/ or downloads/restore . Allows Upload Choose Yes if this service allows you to upload files, No otherwise. Info This setting is important. When you run a job, DART gives you a choice of storage services to which to send your files. DART will show only those services where Allows Upload is set to Yes . Allows Download Choose Yes if this service allows you to download files, No otherwise. Info While DART does not support downloads in its initial release, it may support them in a future release. Login Enter your login name for the service. For FTP and rsync services, this will typically be a user name. For S3 services, it will be an access key ID. For S3 services, you may want to keep your access keys in an environment variable. If you choose to do so, you can enter env: followed by the name of the environment variable here and DART will pull the setting from the environment at run time. For example, if you keep your AWS access key id in an environment variable called AWS_ACCESS_KEY_ID, then enter env:AWS_ACCESS_KEY_ID . Info Environment variables beginning with env: work only when you launch DART from the command line. They don't work when you launch by clicking the DART icon. Password Enter your password for the service. For FTP and rsync services, this will typically be an actual passowrd. For S3 services, it will be a secret access key. As with the Login field above, you can set this to reference an environment variable using the env: pattern. For example, env:AWS_SECRET_ACCESS_KEY . Login Extra This field is typically not used. If your storage service requires it, the plugin documentation should describe what to enter here. Otherwise, leave this field blank.","title":"Storage Services"},{"location":"users/settings/storage_services/#storage-services","text":"Storage services are not repositories! They are pickup and drop-off points for materials going into or coming out of repositories. Some repositories ask depositors to upload materials into an S3 bucket or an SFTP folder for ingest, and restore materials to a similar bucket or folder for depositors to retrieve. Storage services allow DART to connect to these pickup and drop-off points. Note, however, that you're free to send data to and from these storage services even if they're not ultimately bound for a preservation repository.","title":"Storage Services"},{"location":"users/settings/storage_services/#listing-storage-services","text":"To list all storage services, choose Settings > Storage Services from the menu.","title":"Listing Storage Services"},{"location":"users/settings/storage_services/#editing-storage-services","text":"Click any storage servicei in the list to edit it.","title":"Editing Storage Services"},{"location":"users/settings/storage_services/#description-of-settings","text":"","title":"Description of Settings"},{"location":"users/settings/storage_services/#name","text":"The name of the service. Choose a name that's meaningful to you and that differentiates this service from others. You can change the name at any time without affecting the bevavior or availability of the service.","title":"Name"},{"location":"users/settings/storage_services/#description","text":"A description of this service.","title":"Description"},{"location":"users/settings/storage_services/#protocol","text":"Choose the network protocol used to communicate with this service. Note: At launch, DART supports only the S3 protocol.","title":"Protocol"},{"location":"users/settings/storage_services/#host","text":"Enter the name or IP address of the service host. Do not include protocol prefixes like https:// or ftp:// . For example, the host name for Amazon's S3 service is s3.amazonaws.com . A locally hosted service may be s3.example.com or ftp.example.com . You can also enter an IP address here such as 127.0.0.1 .","title":"Host"},{"location":"users/settings/storage_services/#port","text":"The port to connect to. In most cases, you'll want to leave this at 0 (zero). Set this only if the service is running on a non-standard port number.","title":"Port"},{"location":"users/settings/storage_services/#bucket","text":"The name of the bucket you'll be uploading into or downloading from on the remote host. For the S3 protocol, this will be a bucket name like aptrust.dart.test . (If you are an APTrust member, your bucket name will be aptrust.receiving.test. for the demo system and aptrust.receiving. for the production system. For example, receiving buckets for the University of Virginia would be aptrust.test.receiving.virginia.edu for demo and aptrust.receiving.virginia.edu for production.) For protocols like FTP and rsync, this Bucket setting should be a directory name like uploads/ingest/ or downloads/restore .","title":"Bucket"},{"location":"users/settings/storage_services/#allows-upload","text":"Choose Yes if this service allows you to upload files, No otherwise. Info This setting is important. When you run a job, DART gives you a choice of storage services to which to send your files. DART will show only those services where Allows Upload is set to Yes .","title":"Allows Upload"},{"location":"users/settings/storage_services/#allows-download","text":"Choose Yes if this service allows you to download files, No otherwise. Info While DART does not support downloads in its initial release, it may support them in a future release.","title":"Allows Download"},{"location":"users/settings/storage_services/#login","text":"Enter your login name for the service. For FTP and rsync services, this will typically be a user name. For S3 services, it will be an access key ID. For S3 services, you may want to keep your access keys in an environment variable. If you choose to do so, you can enter env: followed by the name of the environment variable here and DART will pull the setting from the environment at run time. For example, if you keep your AWS access key id in an environment variable called AWS_ACCESS_KEY_ID, then enter env:AWS_ACCESS_KEY_ID . Info Environment variables beginning with env: work only when you launch DART from the command line. They don't work when you launch by clicking the DART icon.","title":"Login"},{"location":"users/settings/storage_services/#password","text":"Enter your password for the service. For FTP and rsync services, this will typically be an actual passowrd. For S3 services, it will be a secret access key. As with the Login field above, you can set this to reference an environment variable using the env: pattern. For example, env:AWS_SECRET_ACCESS_KEY .","title":"Password"},{"location":"users/settings/storage_services/#login-extra","text":"This field is typically not used. If your storage service requires it, the plugin documentation should describe what to enter here. Otherwise, leave this field blank.","title":"Login Extra"},{"location":"users/workflows/","text":"Workflows A workflow is a set of packaging, validation, and/or upload operations that you define as a template to be run on any sets of files you choose. Workflows ensure that the exact same set of steps is run on each set of files. DART can run workflows from the UI and from the command line, which means you can create scripts that use workflows to package and upload materials. Note In the future, DART plans to support running workflows on servers. Currently, this is difficult because DART's underlying Electron framework insists on graphics capabilities even when running in command-line mode. We plan to release a stand-alone binary that will be able to run DART jobs and workflows on a server without requiring a UI or graphics capabilities. You'll find a draft of the proposed features here . You can provide feedback through the survey at the end of the features document. Defining a Workflow DART provides two ways to define a workflow: from a job or from scratch. Creating a Workflow from a Job The easiest way to create a workflow is to first create a job that includes all of the operations you want to include, run the job to ensure it works, then click the Create Workflow button on the Review and Run screen. Creating a Workflow from Scratch To create a workflow from scratch: Choose Workflows > New from the menu. Fill in the details. The Name is required and should describe what the workflow will do. This name will appear on the Workflow menu, and scripted tasks will use this name to reference the workflow. (See #2 under Tips for Workflows below.) The Description is for your internal use. Choose the Package Format that suits your needs. If you chose BagIt as the Package Format, choose which BagIt Profile you want to use to build the bag. (See #1 under Tips for Workflows below.) Choose the destination to which you want to upload the packaged files. Note that only Storage Services where Allows Upload is set to Yes will appear in the Upload To list. Click Save , and remember that you can edit or delete this workflow later. Tips for Workflows Use customized BagIt profiles. DART lets you clone the base version of a BagIt profile and then add your own tag default values, so that you don't have to re-enter those values each time you create a new bag. For example, most BagIt profiles require a value for the Source-Organization tag, and typically, this value will be the same for every bag you create. Most profiles also require a number of other tags that may rarely or never change from bag to bag. For example, most depositors will set the same Access and Storage-Option values for 99% of the bags they upload to APTrust. Including in your workflow a custom BagIt profile with pre-set default values will save you having to re-enter common tag values, while still allowing you to override them when necessary. Use meaningful names for your workflows. Many repositories include both a staging environment and a production environment. You may create workflows that are identical except that one pushes packages into the staging environment while the other pushes to production. DART lets you run workflows directly from the workflow menu. Meaningful names help uses choose the right workflow and understand what the consequences of the actions they are about to take. Exporting a Workflow You can export DART workflows to run on a server using dart-runner . To export a workflow: Choose Workflows > List from the DART menu. Click on the workflow you want to export. Click the blue Export button near the top right of the Workflow page. Note You may see a warning about unencrypted passwords included in the export. You can ignore this if you're copying the workflow directly to a server you trust. However, you should use environment variables instead of embedded, plain-text passwords when sending workflows to others via insecure networks. See Storage Service passwords for info on how to use environment variables. After clicking Export , click Copy to Clipboard to copy the JSON.","title":"Workflows"},{"location":"users/workflows/#workflows","text":"A workflow is a set of packaging, validation, and/or upload operations that you define as a template to be run on any sets of files you choose. Workflows ensure that the exact same set of steps is run on each set of files. DART can run workflows from the UI and from the command line, which means you can create scripts that use workflows to package and upload materials. Note In the future, DART plans to support running workflows on servers. Currently, this is difficult because DART's underlying Electron framework insists on graphics capabilities even when running in command-line mode. We plan to release a stand-alone binary that will be able to run DART jobs and workflows on a server without requiring a UI or graphics capabilities. You'll find a draft of the proposed features here . You can provide feedback through the survey at the end of the features document.","title":"Workflows"},{"location":"users/workflows/#defining-a-workflow","text":"DART provides two ways to define a workflow: from a job or from scratch.","title":"Defining a Workflow"},{"location":"users/workflows/#creating-a-workflow-from-a-job","text":"The easiest way to create a workflow is to first create a job that includes all of the operations you want to include, run the job to ensure it works, then click the Create Workflow button on the Review and Run screen.","title":"Creating a Workflow from a Job"},{"location":"users/workflows/#creating-a-workflow-from-scratch","text":"To create a workflow from scratch: Choose Workflows > New from the menu. Fill in the details. The Name is required and should describe what the workflow will do. This name will appear on the Workflow menu, and scripted tasks will use this name to reference the workflow. (See #2 under Tips for Workflows below.) The Description is for your internal use. Choose the Package Format that suits your needs. If you chose BagIt as the Package Format, choose which BagIt Profile you want to use to build the bag. (See #1 under Tips for Workflows below.) Choose the destination to which you want to upload the packaged files. Note that only Storage Services where Allows Upload is set to Yes will appear in the Upload To list. Click Save , and remember that you can edit or delete this workflow later.","title":"Creating a Workflow from Scratch"},{"location":"users/workflows/#tips-for-workflows","text":"Use customized BagIt profiles. DART lets you clone the base version of a BagIt profile and then add your own tag default values, so that you don't have to re-enter those values each time you create a new bag. For example, most BagIt profiles require a value for the Source-Organization tag, and typically, this value will be the same for every bag you create. Most profiles also require a number of other tags that may rarely or never change from bag to bag. For example, most depositors will set the same Access and Storage-Option values for 99% of the bags they upload to APTrust. Including in your workflow a custom BagIt profile with pre-set default values will save you having to re-enter common tag values, while still allowing you to override them when necessary. Use meaningful names for your workflows. Many repositories include both a staging environment and a production environment. You may create workflows that are identical except that one pushes packages into the staging environment while the other pushes to production. DART lets you run workflows directly from the workflow menu. Meaningful names help uses choose the right workflow and understand what the consequences of the actions they are about to take.","title":"Tips for Workflows"},{"location":"users/workflows/#exporting-a-workflow","text":"You can export DART workflows to run on a server using dart-runner . To export a workflow: Choose Workflows > List from the DART menu. Click on the workflow you want to export. Click the blue Export button near the top right of the Workflow page. Note You may see a warning about unencrypted passwords included in the export. You can ignore this if you're copying the workflow directly to a server you trust. However, you should use environment variables instead of embedded, plain-text passwords when sending workflows to others via insecure networks. See Storage Service passwords for info on how to use environment variables. After clicking Export , click Copy to Clipboard to copy the JSON.","title":"Exporting a Workflow"},{"location":"users/workflows/batch_jobs/","text":"Batch Jobs Batch jobs allow you to define a series of jobs in a spreadsheet and then run all of the jobs through the same workflow. Note that a workflow typically includes a BagIt profile, often with default tag values already filled in, and one or more upload targets. When you run a batch of jobs through a workflow, all steps of that workflow are applied to every item in the batch. For example, that could mean bagging up 1000 directories using the APTrust BagIt profile and sending them all to APTrust's demo or production environment. Batch jobs are not limited to APTrust. Your bags can conform to any BagIt profile and be uploaded to any SFTP or S3-compliant server. Batch Spreadsheet Requirements Your spreadsheet must meet the following requirements: The first line must contain column names. The sheet must contain the following columns: Bag-Name - The name of the bag you wish to create. Root-Directory - The absolute path to the directory containing the files you want to bag. You must include one column for each required tag in tag in your workflow's BagIt profile. The column header for each tag must be in the format tag_file_name.txt/tag-name . For example, bag-info.txt/Source-Organization , or aptrust-info.txt/Storage-Option . Your file may omit tags whose default values are already defined in your BagIt profile. (See Customizing a Tag for more info on how to set default tag values in your bagging profiles.) Your file may include extra tags not defined in your workflow's BagIt profile. For example, if you include a column named bag-info.txt/Custom-Tag , DART will add the tag Custom-Tag to the bag-info.txt file of each bag, along with the value specified in that bag's row. Adding custom-tag-file.txt/My-Tag will cause DART to create a tag file for each bag called custom-tag-file.txt and add My-Tag to it, along with the appropriate value. You must save your spreadsheet as a CSV (comma separated value) file so DART can read it. Both Microsoft Excel and Google Sheets can export to CSV format. Example Spreadsheet The sample spreadsheet below defines a batch job to bag up DART source files. Note that it includes definitions for tags required by the APTrust BagIt profile, including the Source-Organization tag in bag-info.txt and the Title, Description, Access, and Storage-Option tags in aptrust-info.txt. Use the right and left arrow keys to scroll sideways in the sheet. You'll see a custom tag at the end called bag-info.txt/Custom-Tag . This tag is not defined in any BagIt profile, but DART will add it to all the bags in the batch and the bag will still be valid because BagIt profiles define only what must be present and do not prohibit what may be present. Running a Batch Job 1. Choose Run Batch from the Workflows menu. 2. Select a Workflow from the list and choose a CSV file. The image below shows the Send to Staging workflow and the CSV file /Users/apd4n/Desktop/test_batch.csv . 3. Click the Run button. DART will show the progress of each job as it runs, and will show the outcome of each job as it completes. In the image below, all nine jobs completed successfully. Notes on Batch Jobs DART runs each job in a batch as a separate process. DART runs jobs one at a time because bagging involves a lot of disk reads and writes. If you try to run a job with an invalid workflow, or your CSV file points to directories and files that don't exist, DART will display the errors and will not run the batch. Individual jobs within the batch may fail. The most common reason for failure is lack of file permissions on one or more files to be bagged. If a job fails, DART will display the reason for failure and will continue running the other jobs in the batch. You must stay on the Workflow Batch screen until all jobs in the batch are complete. If you navigate away from that screen, DART will complete the currently-running job but will not run the remaning jobs in the CSV file. Common Problems DART complains about missing or empty values in the CSV file. First, make sure the values are actually present in the file. Excel sometimes exports a number of empty rows at the end of a CSV file. If DART complains about missing values, open the CSV file in a text editor and delete the empty rows. These often appear as lines containing nothing but commas. DART says required tag values are missing, but they appear to be in the CSV file. Make sure the column headers are correct. A common error is to have headers like bag-info/Source-Organization instead of bag-info.txt/Source-Organization . The .txt is required to match the tag from the CSV file to the tag in your workflow's BagIt profile. You should also ensure that capitalization and hyphenation match. DART says a job failed because of EACCESS Cannot read <some file> . Check the file permissions on the file DART complained about. Although you may have read permissions on most of the files you're trying to bag, you may be lacking permissions on one or two. That's enough to cause the job to fail. If you're bagging files from a shared network drive, you may have to ask a system administrator for help in setting file permissions. Note Batch jobs are new in DART 2.0.9 (released July 24, 2020). If you find bugs, please report them to the DART GitHub repostory or to help@aptrust.org .","title":"Batch Jobs"},{"location":"users/workflows/batch_jobs/#batch-jobs","text":"Batch jobs allow you to define a series of jobs in a spreadsheet and then run all of the jobs through the same workflow. Note that a workflow typically includes a BagIt profile, often with default tag values already filled in, and one or more upload targets. When you run a batch of jobs through a workflow, all steps of that workflow are applied to every item in the batch. For example, that could mean bagging up 1000 directories using the APTrust BagIt profile and sending them all to APTrust's demo or production environment. Batch jobs are not limited to APTrust. Your bags can conform to any BagIt profile and be uploaded to any SFTP or S3-compliant server.","title":"Batch Jobs"},{"location":"users/workflows/batch_jobs/#batch-spreadsheet-requirements","text":"Your spreadsheet must meet the following requirements: The first line must contain column names. The sheet must contain the following columns: Bag-Name - The name of the bag you wish to create. Root-Directory - The absolute path to the directory containing the files you want to bag. You must include one column for each required tag in tag in your workflow's BagIt profile. The column header for each tag must be in the format tag_file_name.txt/tag-name . For example, bag-info.txt/Source-Organization , or aptrust-info.txt/Storage-Option . Your file may omit tags whose default values are already defined in your BagIt profile. (See Customizing a Tag for more info on how to set default tag values in your bagging profiles.) Your file may include extra tags not defined in your workflow's BagIt profile. For example, if you include a column named bag-info.txt/Custom-Tag , DART will add the tag Custom-Tag to the bag-info.txt file of each bag, along with the value specified in that bag's row. Adding custom-tag-file.txt/My-Tag will cause DART to create a tag file for each bag called custom-tag-file.txt and add My-Tag to it, along with the appropriate value. You must save your spreadsheet as a CSV (comma separated value) file so DART can read it. Both Microsoft Excel and Google Sheets can export to CSV format.","title":"Batch Spreadsheet Requirements"},{"location":"users/workflows/batch_jobs/#example-spreadsheet","text":"The sample spreadsheet below defines a batch job to bag up DART source files. Note that it includes definitions for tags required by the APTrust BagIt profile, including the Source-Organization tag in bag-info.txt and the Title, Description, Access, and Storage-Option tags in aptrust-info.txt. Use the right and left arrow keys to scroll sideways in the sheet. You'll see a custom tag at the end called bag-info.txt/Custom-Tag . This tag is not defined in any BagIt profile, but DART will add it to all the bags in the batch and the bag will still be valid because BagIt profiles define only what must be present and do not prohibit what may be present.","title":"Example Spreadsheet"},{"location":"users/workflows/batch_jobs/#running-a-batch-job","text":"1. Choose Run Batch from the Workflows menu. 2. Select a Workflow from the list and choose a CSV file. The image below shows the Send to Staging workflow and the CSV file /Users/apd4n/Desktop/test_batch.csv . 3. Click the Run button. DART will show the progress of each job as it runs, and will show the outcome of each job as it completes. In the image below, all nine jobs completed successfully.","title":"Running a Batch Job"},{"location":"users/workflows/batch_jobs/#notes-on-batch-jobs","text":"DART runs each job in a batch as a separate process. DART runs jobs one at a time because bagging involves a lot of disk reads and writes. If you try to run a job with an invalid workflow, or your CSV file points to directories and files that don't exist, DART will display the errors and will not run the batch. Individual jobs within the batch may fail. The most common reason for failure is lack of file permissions on one or more files to be bagged. If a job fails, DART will display the reason for failure and will continue running the other jobs in the batch. You must stay on the Workflow Batch screen until all jobs in the batch are complete. If you navigate away from that screen, DART will complete the currently-running job but will not run the remaning jobs in the CSV file.","title":"Notes on Batch Jobs"},{"location":"users/workflows/batch_jobs/#common-problems","text":"DART complains about missing or empty values in the CSV file. First, make sure the values are actually present in the file. Excel sometimes exports a number of empty rows at the end of a CSV file. If DART complains about missing values, open the CSV file in a text editor and delete the empty rows. These often appear as lines containing nothing but commas. DART says required tag values are missing, but they appear to be in the CSV file. Make sure the column headers are correct. A common error is to have headers like bag-info/Source-Organization instead of bag-info.txt/Source-Organization . The .txt is required to match the tag from the CSV file to the tag in your workflow's BagIt profile. You should also ensure that capitalization and hyphenation match. DART says a job failed because of EACCESS Cannot read <some file> . Check the file permissions on the file DART complained about. Although you may have read permissions on most of the files you're trying to bag, you may be lacking permissions on one or two. That's enough to cause the job to fail. If you're bagging files from a shared network drive, you may have to ask a system administrator for help in setting file permissions. Note Batch jobs are new in DART 2.0.9 (released July 24, 2020). If you find bugs, please report them to the DART GitHub repostory or to help@aptrust.org .","title":"Common Problems"},{"location":"users/workflows/job_params/","text":"Job Params The JobParams object is a simple JSON structure that gives a DART command-line job four essential pieces of information it needs to run a job. These are: workflowName - The name of the workflow to run. packageName - The name of the package to create. files - A list of files and/or directories to put into the package and/or upload to a remote server. If the workflow includes one or more upload operations but no packaging operation, the files will be uploaded directly. tags - A list of tag names and values to add to the package. Below is sample JobParams JSON describing which files should be passed through the \"APTrust Demo System Workflow\" and what tag values should be applied. In this case, two directories and one file will be packaged with four custom-defined tags. { \"workflowName\" : \"APTrust Demo System Workflow\" , \"packageName\" : \"test.edu.my_files.tar\" , \"files\" : [ \"/path/to/first/directory\" , \"/path/to/second/directory\" , \"/path/to/some/document.pdf\" ], \"tags\" : [ { \"tagFile\" : \"bag-info.txt\" , \"tagName\" : \"Bag-Group-Identifier\" , \"userValue\" : \"Photos_2019\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Title\" , \"userValue\" : \"Photos from 2019\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Description\" , \"userValue\" : \"What I did with my summer vacation.\" }, { \"tagFile\" : \"custom/legal-info.txt\" , \"tagName\" : \"License\" , \"userValue\" : \"https://creativecommons.org/publicdomain/zero/1.0/\" } ] } Let's assume the hypothetical \"APTrust Demo System Workflow\" includes the following steps: Package all files according to our customized version of the APTrust BagIt profile. Validate the resulting bag. Upload the bag to our ingest bucket for the APTrust Demo system. Upload the bag to the S3 bucket in our local owncloud S3 service. Passing the JSON above to the DART command line application will result in the following: The file /path/to/some/document.pdf and the contents of the directories /path/to/first/directory and /path/to/second/directory will be packed into the data directory of a new BagIt bag. DART will create and populate all of the tag files required by the APTrust BagIt profile. It will use the default values saved in your locally customized version of the profile where possible, overriding the defaults with the bag-specific values supplied in the JSON. Note Note that certain BagIt tag values, such as Source-Organization, Contact-Email, etc. will not change from bag to bag, so it makes sense to set default values for them in your customized profile. Other values, such as bag title and description, will change for every bag, so it makes sense to set them in the JobParams JSON. DART will save the entire package to a file whose name matches the package name. DART will write the file into your bagging directory, which is usually a directory called .dart under your home directory. You can verify (and change) the name of your bagging directory by choosing App Settings from the DART menu and clicking on the setting called Bagging Directory . DART will verify the bag according to the APTrust BagIt profile (or whichever profile the Workflow specified). DART will upload the verified bag to APTrust's S3 ingest bucket. DART will upload the verified bag to your local ownCloud S3 bucket. See also: Workflows , Command Line Reference , Scripting with DART Further Reading You'll find more detailed developer documentation for the JobParams object in our DART API documentation .","title":"Job Params"},{"location":"users/workflows/job_params/#job-params","text":"The JobParams object is a simple JSON structure that gives a DART command-line job four essential pieces of information it needs to run a job. These are: workflowName - The name of the workflow to run. packageName - The name of the package to create. files - A list of files and/or directories to put into the package and/or upload to a remote server. If the workflow includes one or more upload operations but no packaging operation, the files will be uploaded directly. tags - A list of tag names and values to add to the package. Below is sample JobParams JSON describing which files should be passed through the \"APTrust Demo System Workflow\" and what tag values should be applied. In this case, two directories and one file will be packaged with four custom-defined tags. { \"workflowName\" : \"APTrust Demo System Workflow\" , \"packageName\" : \"test.edu.my_files.tar\" , \"files\" : [ \"/path/to/first/directory\" , \"/path/to/second/directory\" , \"/path/to/some/document.pdf\" ], \"tags\" : [ { \"tagFile\" : \"bag-info.txt\" , \"tagName\" : \"Bag-Group-Identifier\" , \"userValue\" : \"Photos_2019\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Title\" , \"userValue\" : \"Photos from 2019\" }, { \"tagFile\" : \"aptrust-info.txt\" , \"tagName\" : \"Description\" , \"userValue\" : \"What I did with my summer vacation.\" }, { \"tagFile\" : \"custom/legal-info.txt\" , \"tagName\" : \"License\" , \"userValue\" : \"https://creativecommons.org/publicdomain/zero/1.0/\" } ] } Let's assume the hypothetical \"APTrust Demo System Workflow\" includes the following steps: Package all files according to our customized version of the APTrust BagIt profile. Validate the resulting bag. Upload the bag to our ingest bucket for the APTrust Demo system. Upload the bag to the S3 bucket in our local owncloud S3 service. Passing the JSON above to the DART command line application will result in the following: The file /path/to/some/document.pdf and the contents of the directories /path/to/first/directory and /path/to/second/directory will be packed into the data directory of a new BagIt bag. DART will create and populate all of the tag files required by the APTrust BagIt profile. It will use the default values saved in your locally customized version of the profile where possible, overriding the defaults with the bag-specific values supplied in the JSON. Note Note that certain BagIt tag values, such as Source-Organization, Contact-Email, etc. will not change from bag to bag, so it makes sense to set default values for them in your customized profile. Other values, such as bag title and description, will change for every bag, so it makes sense to set them in the JobParams JSON. DART will save the entire package to a file whose name matches the package name. DART will write the file into your bagging directory, which is usually a directory called .dart under your home directory. You can verify (and change) the name of your bagging directory by choosing App Settings from the DART menu and clicking on the setting called Bagging Directory . DART will verify the bag according to the APTrust BagIt profile (or whichever profile the Workflow specified). DART will upload the verified bag to APTrust's S3 ingest bucket. DART will upload the verified bag to your local ownCloud S3 bucket. See also: Workflows , Command Line Reference , Scripting with DART","title":"Job Params"},{"location":"users/workflows/job_params/#further-reading","text":"You'll find more detailed developer documentation for the JobParams object in our DART API documentation .","title":"Further Reading"}]}